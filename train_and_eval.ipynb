{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b22c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils.MNIST_loaders import MNIST_loaders\n",
    "\n",
    "from models.HypernetClassifier import HyperNetClassifier\n",
    "from models.Classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0e57248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters (things you can easily change!) ---\n",
    "num_epochs = 12\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "optimizer = 'Adam'\n",
    "batch_size = 64\n",
    "validation_split = 0.2  # Percentage of the training data to use for validation\n",
    "random_seed = 42      # For making sure our splits are the same each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54f26b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0016ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    hypernet_model.train()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    hypernet_model = hypernet_model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    hypernet_total_loss = 0\n",
    "    hypernet_correct = 0\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypernet_optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        hypernet_output = hypernet_model(data)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        hypernet_loss = nn.CrossEntropyLoss()(hypernet_output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        hypernet_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        hypernet_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        hypernet_total_loss += hypernet_loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        _, hypernet_predicted = torch.max(hypernet_output.data, 1)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        \n",
    "        correct += (predicted == target).sum().item()\n",
    "        hypernet_correct += (hypernet_predicted == target).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}. Hypernet Loss: {hypernet_loss.item():.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_hypernet_loss = hypernet_total_loss / len(train_loader)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    hypernet_accuracy = 100. * hypernet_correct / total\n",
    "    \n",
    "    print(f'Train Epoch: {epoch} Average Loss: {avg_loss:.4f}, Average Hypernet Loss: {avg_hypernet_loss:.4f}, Accuracy: {accuracy:.2f}%, Hypernet Accuracy: {hypernet_accuracy:.2f}%')\n",
    "    return avg_loss, avg_hypernet_loss, accuracy, hypernet_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1546f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # For ECE calculation\n",
    "    n_bins = 10  # Since it's a 10-class classification task\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1).to(device)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    bin_corrects = torch.zeros(n_bins).to(device)\n",
    "    bin_totals = torch.zeros(n_bins).to(device)\n",
    "    bin_confidences = torch.zeros(n_bins).to(device)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            # Calculate probabilities and confidences for ECE\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            confidences, predictions = torch.max(probabilities, dim=1)\n",
    "            accuracies = predictions.eq(target)\n",
    "\n",
    "            for i in range(n_bins):\n",
    "                in_bin = (confidences >= bin_lowers[i]) & (confidences < bin_uppers[i])\n",
    "                bin_totals[i] += in_bin.sum().item()\n",
    "                bin_corrects[i] += (accuracies[in_bin]).sum().item()\n",
    "                bin_confidences[i] += (confidences[in_bin]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    # Calculate ECE\n",
    "    ece = torch.zeros(1, device=device)\n",
    "    for i in range(n_bins):\n",
    "        if bin_totals[i] > 0:\n",
    "            bin_accuracy = bin_corrects[i] / bin_totals[i]\n",
    "            avg_confidence = bin_confidences[i] / bin_totals[i]\n",
    "            ece += torch.abs(avg_confidence - bin_accuracy) * bin_totals[i]\n",
    "    ece = ece / total\n",
    "    ece = ece.item()\n",
    "\n",
    "    return avg_loss, accuracy, ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20b630e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    test_loss, test_accuracy, ece = evaluate(model, test_loader)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, ECE: {ece:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a493342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "input_size = 28 * 28\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8f0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Train Epoch: 1 [6336/48000 (13%)]\tLoss: 1.504349. Hypernet Loss: 0.825212\n",
      "Train Epoch: 1 [12736/48000 (27%)]\tLoss: 0.980619. Hypernet Loss: 0.435239\n",
      "Train Epoch: 1 [19136/48000 (40%)]\tLoss: 0.929028. Hypernet Loss: 0.461207\n",
      "Train Epoch: 1 [25536/48000 (53%)]\tLoss: 0.744850. Hypernet Loss: 0.333102\n",
      "Train Epoch: 1 [31936/48000 (67%)]\tLoss: 0.448523. Hypernet Loss: 0.179546\n",
      "Train Epoch: 1 [38336/48000 (80%)]\tLoss: 0.458848. Hypernet Loss: 0.320934\n",
      "Train Epoch: 1 [44736/48000 (93%)]\tLoss: 0.515582. Hypernet Loss: 0.247802\n",
      "Train Epoch: 1 Average Loss: 0.9089, Average Hypernet Loss: 0.3930, Accuracy: 77.24%, Hypernet Accuracy: 88.35%\n",
      "Validation Epoch: 1 Average Loss: 0.5324, Accuracy: 86.27%, ECE: 0.14255\n",
      "Hypernet Validation Epoch: 1 Average Loss: 0.3352, Accuracy: 90.30%, ECE: 0.00568\n",
      "Train Epoch: 2 [6336/48000 (13%)]\tLoss: 0.501133. Hypernet Loss: 0.291144\n",
      "Train Epoch: 2 [12736/48000 (27%)]\tLoss: 0.469759. Hypernet Loss: 0.233299\n",
      "Train Epoch: 2 [19136/48000 (40%)]\tLoss: 0.561288. Hypernet Loss: 0.448457\n",
      "Train Epoch: 2 [25536/48000 (53%)]\tLoss: 0.346240. Hypernet Loss: 0.149376\n",
      "Train Epoch: 2 [31936/48000 (67%)]\tLoss: 0.446746. Hypernet Loss: 0.264704\n",
      "Train Epoch: 2 [38336/48000 (80%)]\tLoss: 0.489650. Hypernet Loss: 0.431086\n",
      "Train Epoch: 2 [44736/48000 (93%)]\tLoss: 0.328606. Hypernet Loss: 0.207329\n",
      "Train Epoch: 2 Average Loss: 0.4539, Average Hypernet Loss: 0.3136, Accuracy: 88.23%, Hypernet Accuracy: 91.04%\n",
      "Validation Epoch: 2 Average Loss: 0.4148, Accuracy: 88.70%, ECE: 0.08511\n",
      "Hypernet Validation Epoch: 2 Average Loss: 0.3268, Accuracy: 91.10%, ECE: 0.00902\n",
      "Train Epoch: 3 [6336/48000 (13%)]\tLoss: 0.338937. Hypernet Loss: 0.211570\n",
      "Train Epoch: 3 [12736/48000 (27%)]\tLoss: 0.352503. Hypernet Loss: 0.223373\n",
      "Train Epoch: 3 [19136/48000 (40%)]\tLoss: 0.266191. Hypernet Loss: 0.186995\n",
      "Train Epoch: 3 [25536/48000 (53%)]\tLoss: 0.490432. Hypernet Loss: 0.497684\n",
      "Train Epoch: 3 [31936/48000 (67%)]\tLoss: 0.319446. Hypernet Loss: 0.177959\n",
      "Train Epoch: 3 [38336/48000 (80%)]\tLoss: 0.304285. Hypernet Loss: 0.261526\n",
      "Train Epoch: 3 [44736/48000 (93%)]\tLoss: 0.517749. Hypernet Loss: 0.532196\n",
      "Train Epoch: 3 Average Loss: 0.3786, Average Hypernet Loss: 0.2950, Accuracy: 89.65%, Hypernet Accuracy: 91.70%\n",
      "Validation Epoch: 3 Average Loss: 0.3716, Accuracy: 89.58%, ECE: 0.06087\n",
      "Hypernet Validation Epoch: 3 Average Loss: 0.3288, Accuracy: 90.78%, ECE: 0.00549\n",
      "Train Epoch: 4 [6336/48000 (13%)]\tLoss: 0.270662. Hypernet Loss: 0.272069\n",
      "Train Epoch: 4 [12736/48000 (27%)]\tLoss: 0.566161. Hypernet Loss: 0.617290\n",
      "Train Epoch: 4 [19136/48000 (40%)]\tLoss: 0.315344. Hypernet Loss: 0.229172\n",
      "Train Epoch: 4 [25536/48000 (53%)]\tLoss: 0.325804. Hypernet Loss: 0.331914\n",
      "Train Epoch: 4 [31936/48000 (67%)]\tLoss: 0.301486. Hypernet Loss: 0.220408\n",
      "Train Epoch: 4 [38336/48000 (80%)]\tLoss: 0.255563. Hypernet Loss: 0.136046\n",
      "Train Epoch: 4 [44736/48000 (93%)]\tLoss: 0.379768. Hypernet Loss: 0.319634\n",
      "Train Epoch: 4 Average Loss: 0.3445, Average Hypernet Loss: 0.2889, Accuracy: 90.40%, Hypernet Accuracy: 91.79%\n",
      "Validation Epoch: 4 Average Loss: 0.3492, Accuracy: 90.21%, ECE: 0.04913\n",
      "Hypernet Validation Epoch: 4 Average Loss: 0.3279, Accuracy: 91.02%, ECE: 0.00388\n",
      "Train Epoch: 5 [6336/48000 (13%)]\tLoss: 0.205609. Hypernet Loss: 0.168699\n",
      "Train Epoch: 5 [12736/48000 (27%)]\tLoss: 0.308543. Hypernet Loss: 0.302536\n",
      "Train Epoch: 5 [19136/48000 (40%)]\tLoss: 0.353961. Hypernet Loss: 0.324342\n",
      "Train Epoch: 5 [25536/48000 (53%)]\tLoss: 0.311683. Hypernet Loss: 0.233100\n",
      "Train Epoch: 5 [31936/48000 (67%)]\tLoss: 0.254173. Hypernet Loss: 0.172169\n",
      "Train Epoch: 5 [38336/48000 (80%)]\tLoss: 0.197965. Hypernet Loss: 0.173726\n",
      "Train Epoch: 5 [44736/48000 (93%)]\tLoss: 0.419448. Hypernet Loss: 0.457373\n",
      "Train Epoch: 5 Average Loss: 0.3247, Average Hypernet Loss: 0.2826, Accuracy: 90.82%, Hypernet Accuracy: 92.15%\n",
      "Validation Epoch: 5 Average Loss: 0.3350, Accuracy: 90.48%, ECE: 0.03934\n",
      "Hypernet Validation Epoch: 5 Average Loss: 0.3143, Accuracy: 91.46%, ECE: 0.00509\n",
      "Train Epoch: 6 [6336/48000 (13%)]\tLoss: 0.378588. Hypernet Loss: 0.335442\n",
      "Train Epoch: 6 [12736/48000 (27%)]\tLoss: 0.298360. Hypernet Loss: 0.248364\n",
      "Train Epoch: 6 [19136/48000 (40%)]\tLoss: 0.142530. Hypernet Loss: 0.102848\n",
      "Train Epoch: 6 [25536/48000 (53%)]\tLoss: 0.309213. Hypernet Loss: 0.219758\n",
      "Train Epoch: 6 [31936/48000 (67%)]\tLoss: 0.307164. Hypernet Loss: 0.330862\n",
      "Train Epoch: 6 [38336/48000 (80%)]\tLoss: 0.327710. Hypernet Loss: 0.290723\n",
      "Train Epoch: 6 [44736/48000 (93%)]\tLoss: 0.396261. Hypernet Loss: 0.285982\n",
      "Train Epoch: 6 Average Loss: 0.3116, Average Hypernet Loss: 0.2784, Accuracy: 91.19%, Hypernet Accuracy: 92.23%\n",
      "Validation Epoch: 6 Average Loss: 0.3261, Accuracy: 90.83%, ECE: 0.03617\n",
      "Hypernet Validation Epoch: 6 Average Loss: 0.3267, Accuracy: 90.89%, ECE: 0.01533\n",
      "Train Epoch: 7 [6336/48000 (13%)]\tLoss: 0.250144. Hypernet Loss: 0.198060\n",
      "Train Epoch: 7 [12736/48000 (27%)]\tLoss: 0.249546. Hypernet Loss: 0.213075\n",
      "Train Epoch: 7 [19136/48000 (40%)]\tLoss: 0.175010. Hypernet Loss: 0.170426\n",
      "Train Epoch: 7 [25536/48000 (53%)]\tLoss: 0.360294. Hypernet Loss: 0.356580\n",
      "Train Epoch: 7 [31936/48000 (67%)]\tLoss: 0.229043. Hypernet Loss: 0.202325\n",
      "Train Epoch: 7 [38336/48000 (80%)]\tLoss: 0.252608. Hypernet Loss: 0.145898\n",
      "Train Epoch: 7 [44736/48000 (93%)]\tLoss: 0.289654. Hypernet Loss: 0.220300\n",
      "Train Epoch: 7 Average Loss: 0.3022, Average Hypernet Loss: 0.2758, Accuracy: 91.44%, Hypernet Accuracy: 92.26%\n",
      "Validation Epoch: 7 Average Loss: 0.3201, Accuracy: 91.02%, ECE: 0.03170\n",
      "Hypernet Validation Epoch: 7 Average Loss: 0.3206, Accuracy: 91.16%, ECE: 0.00791\n",
      "Train Epoch: 8 [6336/48000 (13%)]\tLoss: 0.268258. Hypernet Loss: 0.232598\n",
      "Train Epoch: 8 [12736/48000 (27%)]\tLoss: 0.565759. Hypernet Loss: 0.614241\n",
      "Train Epoch: 8 [19136/48000 (40%)]\tLoss: 0.163190. Hypernet Loss: 0.090589\n",
      "Train Epoch: 8 [25536/48000 (53%)]\tLoss: 0.376809. Hypernet Loss: 0.397902\n",
      "Train Epoch: 8 [31936/48000 (67%)]\tLoss: 0.439515. Hypernet Loss: 0.493925\n",
      "Train Epoch: 8 [38336/48000 (80%)]\tLoss: 0.163218. Hypernet Loss: 0.103710\n",
      "Train Epoch: 8 [44736/48000 (93%)]\tLoss: 0.378817. Hypernet Loss: 0.319200\n",
      "Train Epoch: 8 Average Loss: 0.2952, Average Hypernet Loss: 0.2723, Accuracy: 91.66%, Hypernet Accuracy: 92.29%\n",
      "Validation Epoch: 8 Average Loss: 0.3157, Accuracy: 91.12%, ECE: 0.02928\n",
      "Hypernet Validation Epoch: 8 Average Loss: 0.3157, Accuracy: 91.44%, ECE: 0.00543\n",
      "Train Epoch: 9 [6336/48000 (13%)]\tLoss: 0.267192. Hypernet Loss: 0.201330\n",
      "Train Epoch: 9 [12736/48000 (27%)]\tLoss: 0.334528. Hypernet Loss: 0.217102\n",
      "Train Epoch: 9 [19136/48000 (40%)]\tLoss: 0.162358. Hypernet Loss: 0.130822\n",
      "Train Epoch: 9 [25536/48000 (53%)]\tLoss: 0.180931. Hypernet Loss: 0.172893\n",
      "Train Epoch: 9 [31936/48000 (67%)]\tLoss: 0.215428. Hypernet Loss: 0.206096\n",
      "Train Epoch: 9 [38336/48000 (80%)]\tLoss: 0.233501. Hypernet Loss: 0.211735\n",
      "Train Epoch: 9 [44736/48000 (93%)]\tLoss: 0.444817. Hypernet Loss: 0.460548\n",
      "Train Epoch: 9 Average Loss: 0.2896, Average Hypernet Loss: 0.2687, Accuracy: 91.75%, Hypernet Accuracy: 92.54%\n",
      "Validation Epoch: 9 Average Loss: 0.3128, Accuracy: 91.18%, ECE: 0.02805\n",
      "Hypernet Validation Epoch: 9 Average Loss: 0.3113, Accuracy: 91.47%, ECE: 0.01094\n",
      "Train Epoch: 10 [6336/48000 (13%)]\tLoss: 0.218172. Hypernet Loss: 0.131182\n",
      "Train Epoch: 10 [12736/48000 (27%)]\tLoss: 0.287712. Hypernet Loss: 0.219762\n",
      "Train Epoch: 10 [19136/48000 (40%)]\tLoss: 0.259793. Hypernet Loss: 0.218679\n",
      "Train Epoch: 10 [25536/48000 (53%)]\tLoss: 0.201723. Hypernet Loss: 0.234017\n",
      "Train Epoch: 10 [31936/48000 (67%)]\tLoss: 0.321307. Hypernet Loss: 0.299428\n",
      "Train Epoch: 10 [38336/48000 (80%)]\tLoss: 0.233970. Hypernet Loss: 0.244744\n",
      "Train Epoch: 10 [44736/48000 (93%)]\tLoss: 0.259272. Hypernet Loss: 0.199305\n",
      "Train Epoch: 10 Average Loss: 0.2852, Average Hypernet Loss: 0.2677, Accuracy: 91.96%, Hypernet Accuracy: 92.47%\n",
      "Validation Epoch: 10 Average Loss: 0.3100, Accuracy: 91.38%, ECE: 0.02810\n",
      "Hypernet Validation Epoch: 10 Average Loss: 0.3079, Accuracy: 91.72%, ECE: 0.01704\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(input_size, output_size, dropout=0)\n",
    "hypernet_model = HyperNetClassifier(input_size, output_size, hidden_sizes=[1024, 512, 1024], device=device)\n",
    "\n",
    "if optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    hypernet_optimizer = optim.SGD(hypernet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "elif optim:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    hypernet_optimizer = optim.Adam(hypernet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "train_loader, val_loader, test_loader = MNIST_loaders()\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    _, _, _, _ = train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch)\n",
    "    \n",
    "    val_loss, val_accuracy, ece = evaluate(model, val_loader)\n",
    "    print(f'Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, ECE: {ece:.5f}')\n",
    "    \n",
    "    val_loss, val_accuracy, ece = evaluate(hypernet_model, val_loader)\n",
    "    print(f'Hypernet Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, ECE: {ece:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "554bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Testing...\n",
      "Normal Model:\n",
      "Test set: Average loss: 0.2812, Accuracy: 92.17%, ECE: 0.02945\n",
      "Hypernet Model:\n",
      "Test set: Average loss: 0.2785, Accuracy: 92.17%, ECE: 0.01646\n"
     ]
    }
   ],
   "source": [
    "# --- Testing the Model ---\n",
    "print(\"\\nStarting Testing...\")\n",
    "\n",
    "print(\"Normal Model:\")\n",
    "test(model, test_loader)\n",
    "\n",
    "print(\"Hypernet Model:\")\n",
    "test(hypernet_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
