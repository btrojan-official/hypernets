{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e57248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters (things you can easily change!) ---\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "batch_size = 64\n",
    "validation_split = 0.2  # Percentage of the training data to use for validation\n",
    "random_seed = 42      # For making sure our splits are the same each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f26b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1db3c4dee30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1de5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Preparing the Dataset and DataLoaders ---\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "# Here, we convert the images to PyTorch tensors and normalize the pixel values\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders. These help us load the data in batches during training.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1012917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implementing the Simple Classifier ---\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        # This is a single linear layer (like a simple connection of all inputs to all outputs)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input images are 28x28 pixels, so we need to flatten them into a single vector of 784 elements\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # Pass the flattened vector through the linear layer\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5ce193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28  # 784 input features (28x28 pixels)\n",
    "output_size = 10     # 10 output classes (digits 0-9)\n",
    "model = SimpleClassifier(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0016ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Zero the gradients from the previous batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: compute the output predictions\n",
    "        output = model(data)\n",
    "        # Calculate the loss (how wrong the predictions are compared to the true labels)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        # Backward pass: compute the gradients of the loss with respect to the model's parameters\n",
    "        loss.backward()\n",
    "        # Update the model's parameters based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Train Epoch: {epoch} Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1546f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluating Function ---\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()   # Set the model to evaluation mode (no gradient calculation)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for data, target in data_loader:\n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b630e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Function ---\n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Train Epoch: 1 [6336/48000 (13%)]\tLoss: 0.543875\n",
      "Train Epoch: 1 [12736/48000 (27%)]\tLoss: 0.484451\n",
      "Train Epoch: 1 [19136/48000 (40%)]\tLoss: 0.672292\n",
      "Train Epoch: 1 [25536/48000 (53%)]\tLoss: 0.172376\n",
      "Train Epoch: 1 [31936/48000 (67%)]\tLoss: 0.284131\n",
      "Train Epoch: 1 [38336/48000 (80%)]\tLoss: 0.481687\n",
      "Train Epoch: 1 [44736/48000 (93%)]\tLoss: 0.532227\n",
      "Train Epoch: 1 Average Loss: 0.5105, Accuracy: 87.24%\n",
      "Validation Epoch: 1 Average Loss: 0.5379, Accuracy: 87.69%\n",
      "Train Epoch: 2 [6336/48000 (13%)]\tLoss: 0.100417\n",
      "Train Epoch: 2 [12736/48000 (27%)]\tLoss: 0.749203\n",
      "Train Epoch: 2 [19136/48000 (40%)]\tLoss: 0.419748\n",
      "Train Epoch: 2 [25536/48000 (53%)]\tLoss: 0.630112\n",
      "Train Epoch: 2 [31936/48000 (67%)]\tLoss: 0.523190\n",
      "Train Epoch: 2 [38336/48000 (80%)]\tLoss: 0.388925\n",
      "Train Epoch: 2 [44736/48000 (93%)]\tLoss: 0.626730\n",
      "Train Epoch: 2 Average Loss: 0.5061, Accuracy: 88.51%\n",
      "Validation Epoch: 2 Average Loss: 0.5505, Accuracy: 88.90%\n",
      "Train Epoch: 3 [6336/48000 (13%)]\tLoss: 1.324932\n",
      "Train Epoch: 3 [12736/48000 (27%)]\tLoss: 0.592684\n",
      "Train Epoch: 3 [19136/48000 (40%)]\tLoss: 0.349158\n",
      "Train Epoch: 3 [25536/48000 (53%)]\tLoss: 1.054652\n",
      "Train Epoch: 3 [31936/48000 (67%)]\tLoss: 0.356141\n",
      "Train Epoch: 3 [38336/48000 (80%)]\tLoss: 0.078974\n",
      "Train Epoch: 3 [44736/48000 (93%)]\tLoss: 0.422556\n",
      "Train Epoch: 3 Average Loss: 0.4971, Accuracy: 88.70%\n",
      "Validation Epoch: 3 Average Loss: 0.5177, Accuracy: 88.83%\n",
      "Train Epoch: 4 [6336/48000 (13%)]\tLoss: 0.727923\n",
      "Train Epoch: 4 [12736/48000 (27%)]\tLoss: 0.512815\n",
      "Train Epoch: 4 [19136/48000 (40%)]\tLoss: 0.441319\n",
      "Train Epoch: 4 [25536/48000 (53%)]\tLoss: 0.590609\n",
      "Train Epoch: 4 [31936/48000 (67%)]\tLoss: 0.411797\n",
      "Train Epoch: 4 [38336/48000 (80%)]\tLoss: 0.462814\n",
      "Train Epoch: 4 [44736/48000 (93%)]\tLoss: 1.019455\n",
      "Train Epoch: 4 Average Loss: 0.4908, Accuracy: 88.93%\n",
      "Validation Epoch: 4 Average Loss: 0.4988, Accuracy: 89.12%\n",
      "Train Epoch: 5 [6336/48000 (13%)]\tLoss: 0.916754\n",
      "Train Epoch: 5 [12736/48000 (27%)]\tLoss: 0.827929\n",
      "Train Epoch: 5 [19136/48000 (40%)]\tLoss: 0.438272\n",
      "Train Epoch: 5 [25536/48000 (53%)]\tLoss: 0.643555\n",
      "Train Epoch: 5 [31936/48000 (67%)]\tLoss: 0.696706\n",
      "Train Epoch: 5 [38336/48000 (80%)]\tLoss: 0.290792\n",
      "Train Epoch: 5 [44736/48000 (93%)]\tLoss: 0.612772\n",
      "Train Epoch: 5 Average Loss: 0.5053, Accuracy: 88.58%\n",
      "Validation Epoch: 5 Average Loss: 0.5637, Accuracy: 88.28%\n",
      "Train Epoch: 6 [6336/48000 (13%)]\tLoss: 0.981018\n",
      "Train Epoch: 6 [12736/48000 (27%)]\tLoss: 0.406408\n",
      "Train Epoch: 6 [19136/48000 (40%)]\tLoss: 0.604412\n",
      "Train Epoch: 6 [25536/48000 (53%)]\tLoss: 0.249361\n",
      "Train Epoch: 6 [31936/48000 (67%)]\tLoss: 0.696424\n",
      "Train Epoch: 6 [38336/48000 (80%)]\tLoss: 0.409546\n",
      "Train Epoch: 6 [44736/48000 (93%)]\tLoss: 0.632787\n",
      "Train Epoch: 6 Average Loss: 0.5299, Accuracy: 88.55%\n",
      "Validation Epoch: 6 Average Loss: 0.5402, Accuracy: 88.92%\n",
      "Train Epoch: 7 [6336/48000 (13%)]\tLoss: 0.612142\n",
      "Train Epoch: 7 [12736/48000 (27%)]\tLoss: 0.477903\n",
      "Train Epoch: 7 [19136/48000 (40%)]\tLoss: 0.274732\n",
      "Train Epoch: 7 [25536/48000 (53%)]\tLoss: 0.507488\n",
      "Train Epoch: 7 [31936/48000 (67%)]\tLoss: 0.508101\n",
      "Train Epoch: 7 [38336/48000 (80%)]\tLoss: 0.613674\n",
      "Train Epoch: 7 [44736/48000 (93%)]\tLoss: 0.803325\n",
      "Train Epoch: 7 Average Loss: 0.5009, Accuracy: 88.88%\n",
      "Validation Epoch: 7 Average Loss: 0.5408, Accuracy: 88.54%\n",
      "Train Epoch: 8 [6336/48000 (13%)]\tLoss: 0.522985\n",
      "Train Epoch: 8 [12736/48000 (27%)]\tLoss: 0.680597\n",
      "Train Epoch: 8 [19136/48000 (40%)]\tLoss: 0.476681\n",
      "Train Epoch: 8 [25536/48000 (53%)]\tLoss: 0.502649\n",
      "Train Epoch: 8 [31936/48000 (67%)]\tLoss: 0.282387\n",
      "Train Epoch: 8 [38336/48000 (80%)]\tLoss: 0.778154\n",
      "Train Epoch: 8 [44736/48000 (93%)]\tLoss: 0.517650\n",
      "Train Epoch: 8 Average Loss: 0.5318, Accuracy: 88.53%\n",
      "Validation Epoch: 8 Average Loss: 0.5352, Accuracy: 88.24%\n",
      "Train Epoch: 9 [6336/48000 (13%)]\tLoss: 0.237901\n",
      "Train Epoch: 9 [12736/48000 (27%)]\tLoss: 0.980394\n",
      "Train Epoch: 9 [19136/48000 (40%)]\tLoss: 0.904616\n",
      "Train Epoch: 9 [25536/48000 (53%)]\tLoss: 0.731937\n",
      "Train Epoch: 9 [31936/48000 (67%)]\tLoss: 0.319751\n",
      "Train Epoch: 9 [38336/48000 (80%)]\tLoss: 0.381855\n",
      "Train Epoch: 9 [44736/48000 (93%)]\tLoss: 0.156646\n",
      "Train Epoch: 9 Average Loss: 0.5019, Accuracy: 88.80%\n",
      "Validation Epoch: 9 Average Loss: 0.5974, Accuracy: 87.36%\n",
      "Train Epoch: 10 [6336/48000 (13%)]\tLoss: 0.267394\n",
      "Train Epoch: 10 [12736/48000 (27%)]\tLoss: 1.572035\n",
      "Train Epoch: 10 [19136/48000 (40%)]\tLoss: 0.528835\n",
      "Train Epoch: 10 [25536/48000 (53%)]\tLoss: 0.327127\n",
      "Train Epoch: 10 [31936/48000 (67%)]\tLoss: 0.361719\n",
      "Train Epoch: 10 [38336/48000 (80%)]\tLoss: 0.309854\n",
      "Train Epoch: 10 [44736/48000 (93%)]\tLoss: 0.689093\n",
      "Train Epoch: 10 Average Loss: 0.5140, Accuracy: 88.60%\n",
      "Validation Epoch: 10 Average Loss: 0.5804, Accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, epoch)\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader)\n",
    "    print(f'Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Testing...\n",
      "\n",
      "Test set: Average loss: 0.5573, Accuracy: 88.37%\n"
     ]
    }
   ],
   "source": [
    "# --- Testing the Model ---\n",
    "print(\"\\nStarting Testing...\")\n",
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
