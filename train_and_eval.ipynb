{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b22c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e74ed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0e57248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters (things you can easily change!) ---\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "batch_size = 64\n",
    "validation_split = 0.2  # Percentage of the training data to use for validation\n",
    "random_seed = 42      # For making sure our splits are the same each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54f26b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2bbc05e6e50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1de5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparing the Dataset and DataLoaders ---\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "# Here, we convert the images to PyTorch tensors and normalize the pixel values\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders. These help us load the data in batches during training.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1012917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        # This is a single linear layer (like a simple connection of all inputs to all outputs)\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input images are 28x28 pixels, so we need to flatten them into a single vector of 784 elements\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # Pass the flattened vector through the linear layer\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ee607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[2400, 1200, 2400]):\n",
    "        super(HyperNetClassifier, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.num_weights = input_size * output_size\n",
    "        \n",
    "        self.input_layer = nn.Linear(self.num_weights, hidden_sizes[0], bias=False)\n",
    "        self.hidden_1 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.hidden_2 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[2], self.num_weights, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        mask = (torch.rand(self.num_weights, requires_grad=False) >= 0.5).int().to(device)\n",
    "        hypernet_input = torch.randn(self.num_weights, requires_grad=True).to(device) * mask\n",
    "        \n",
    "        hypernet_output = torch.relu(self.input_layer(hypernet_input))\n",
    "        hypernet_output = torch.relu(self.hidden_1(hypernet_output))\n",
    "        hypernet_output = torch.relu(self.hidden_2(hypernet_output))\n",
    "        hypernet_output = self.output_layer(hypernet_output)\n",
    "\n",
    "        weights = torch.reshape(hypernet_output, (self.input_size, self.output_size))\n",
    "\n",
    "        return x @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "\n",
    "def train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    hypernet_model.train()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    hypernet_model = hypernet_model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    hypernet_total_loss = 0\n",
    "    hypernet_correct = 0\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypernet_optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        hypernet_output = hypernet_model(data)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        hypernet_loss = nn.CrossEntropyLoss()(hypernet_output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        hypernet_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        hypernet_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        hypernet_total_loss += hypernet_loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        _, hypernet_predicted = torch.max(hypernet_output.data, 1)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        \n",
    "        correct += (predicted == target).sum().item()\n",
    "        hypernet_correct += (hypernet_predicted == target).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}. Hypernet Loss: {hypernet_loss.item():.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_hypernet_loss = hypernet_total_loss / len(train_loader)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    hypernet_accuracy = 100. * hypernet_correct / total\n",
    "    \n",
    "    print(f'Train Epoch: {epoch} Average Loss: {avg_loss:.4f}, Average Hypernet Loss: {avg_hypernet_loss:.4f}, Accuracy: {accuracy:.2f}%, Hypernet Accuracy: {hypernet_accuracy:.2f}%')\n",
    "    return avg_loss, avg_hypernet_loss, accuracy, hypernet_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1546f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluating Function ---\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # For ECE calculation\n",
    "    n_bins = 10  # Since it's a 10-class classification task\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1).to(device)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    bin_corrects = torch.zeros(n_bins).to(device)\n",
    "    bin_totals = torch.zeros(n_bins).to(device)\n",
    "    bin_confidences = torch.zeros(n_bins).to(device)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            # Calculate probabilities and confidences for ECE\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            confidences, predictions = torch.max(probabilities, dim=1)\n",
    "            accuracies = predictions.eq(target)\n",
    "\n",
    "            for i in range(n_bins):\n",
    "                in_bin = (confidences >= bin_lowers[i]) & (confidences < bin_uppers[i])\n",
    "                bin_totals[i] += in_bin.sum().item()\n",
    "                bin_corrects[i] += (accuracies[in_bin]).sum().item()\n",
    "                bin_confidences[i] += (confidences[in_bin]).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    # Calculate ECE\n",
    "    ece = torch.zeros(1, device=device)\n",
    "    for i in range(n_bins):\n",
    "        if bin_totals[i] > 0:\n",
    "            bin_accuracy = bin_corrects[i] / bin_totals[i]\n",
    "            avg_confidence = bin_confidences[i] / bin_totals[i]\n",
    "            ece += torch.abs(avg_confidence - bin_accuracy) * bin_totals[i]\n",
    "    ece = ece / total\n",
    "    ece = ece.item()\n",
    "\n",
    "    return avg_loss, accuracy, ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20b630e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Function ---\n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss, test_accuracy, ece = evaluate(model, test_loader)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, ECE: {ece:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a493342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8f0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     val_loss, val_accuracy, ece \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, ECE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mece\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[76], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mhypernet_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\lewy7\\Documents\\GitHub\\hypernets\\.venv\\lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lewy7\\Documents\\GitHub\\hypernets\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\lewy7\\Documents\\GitHub\\hypernets\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:952\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 952\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28  # 784 input features (28x28 pixels)\n",
    "output_size = 10     # 10 output classes (digits 0-9)\n",
    "\n",
    "model = Classifier(input_size, output_size, dropout=0.9)\n",
    "hypernet_model = HyperNetClassifier(input_size, output_size, hidden_sizes=[1024, 512, 1024])\n",
    "\n",
    "# --- Initialize the Optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "hypernet_optimizer = optim.Adam(hypernet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    _, _, _, _ = train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch)\n",
    "    \n",
    "    val_loss, val_accuracy, ece = evaluate(model, val_loader)\n",
    "    print(f'Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, ECE: {ece:.5f}')\n",
    "    \n",
    "    val_loss, val_accuracy, ece = evaluate(hypernet_model, val_loader)\n",
    "    print(f'Hypernet Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, ECE: {ece:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "554bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Testing...\n",
      "Normal Model:\n",
      "Test set: Average loss: 0.5409, Accuracy: 87.87%, ECE: 0.06125\n",
      "Hypernet Model:\n",
      "Test set: Average loss: 48.8385, Accuracy: 9.91%, ECE: 0.00216\n"
     ]
    }
   ],
   "source": [
    "# --- Testing the Model ---\n",
    "print(\"\\nStarting Testing...\")\n",
    "\n",
    "print(\"Normal Model:\")\n",
    "test(model, test_loader)\n",
    "\n",
    "print(\"Hypernet Model:\")\n",
    "test(hypernet_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
