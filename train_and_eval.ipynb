{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74ed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e57248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters (things you can easily change!) ---\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "batch_size = 64\n",
    "validation_split = 0.2  # Percentage of the training data to use for validation\n",
    "random_seed = 42      # For making sure our splits are the same each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f26b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6694d2e30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1de5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparing the Dataset and DataLoaders ---\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "# Here, we convert the images to PyTorch tensors and normalize the pixel values\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders. These help us load the data in batches during training.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1012917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implementing the Simple Classifier ---\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        # This is a single linear layer (like a simple connection of all inputs to all outputs)\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input images are 28x28 pixels, so we need to flatten them into a single vector of 784 elements\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # Pass the flattened vector through the linear layer\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[2400, 1200, 2400]):\n",
    "        super(HyperNetClassifier, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.num_weights = input_size * output_size\n",
    "        \n",
    "        self.input_layer = nn.Linear(self.num_weights, hidden_sizes[0], bias=False)\n",
    "        self.hidden_1 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.hidden_2 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[2], self.num_weights, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        mask = (torch.rand(self.num_weights, requires_grad=False) >= 0.5).int().to(device)\n",
    "        hypernet_input = torch.randn(self.num_weights, requires_grad=True).to(device) * mask\n",
    "        \n",
    "        hypernet_output = torch.relu(self.input_layer(hypernet_input))\n",
    "        hypernet_output = torch.relu(self.hidden_1(hypernet_output))\n",
    "        hypernet_output = torch.relu(self.hidden_2(hypernet_output))\n",
    "        hypernet_output = self.output_layer(hypernet_output)\n",
    "\n",
    "        weights = torch.reshape(hypernet_output, (self.input_size, self.output_size))\n",
    "\n",
    "        return x @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0016ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "\n",
    "def train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    hypernet_model.train()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    hypernet_model = hypernet_model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    hypernet_total_loss = 0\n",
    "    hypernet_correct = 0\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypernet_optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        hypernet_output = hypernet_model(data)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        hypernet_loss = nn.CrossEntropyLoss()(hypernet_output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        hypernet_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        hypernet_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        hypernet_total_loss += hypernet_loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        _, hypernet_predicted = torch.max(hypernet_output.data, 1)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        \n",
    "        correct += (predicted == target).sum().item()\n",
    "        hypernet_correct += (hypernet_predicted == target).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}. Hypernet Loss: {hypernet_loss.item():.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_hypernet_loss = hypernet_total_loss / len(train_loader)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    hypernet_accuracy = 100. * hypernet_correct / total\n",
    "    \n",
    "    print(f'Train Epoch: {epoch} Average Loss: {avg_loss:.4f}, Average Hypernet Loss: {avg_hypernet_loss:.4f}, Accuracy: {accuracy:.2f}%, Hypernet Accuracy: {hypernet_accuracy:.2f}%')\n",
    "    return avg_loss, avg_hypernet_loss, accuracy, hypernet_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1546f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluating Function ---\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20b630e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Function ---\n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a493342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8f0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Train Epoch: 1 [6336/48000 (13%)]\tLoss: 0.646559. Hypernet Loss: 1.850179\n",
      "Train Epoch: 1 [12736/48000 (27%)]\tLoss: 0.573781. Hypernet Loss: 0.382953\n",
      "Train Epoch: 1 [19136/48000 (40%)]\tLoss: 0.340706. Hypernet Loss: 0.229748\n",
      "Train Epoch: 1 [25536/48000 (53%)]\tLoss: 0.643641. Hypernet Loss: 0.387189\n",
      "Train Epoch: 1 [31936/48000 (67%)]\tLoss: 0.595807. Hypernet Loss: 0.542787\n",
      "Train Epoch: 1 [38336/48000 (80%)]\tLoss: 0.376144. Hypernet Loss: 0.360049\n",
      "Train Epoch: 1 [44736/48000 (93%)]\tLoss: 0.704413. Hypernet Loss: 0.456518\n",
      "Train Epoch: 1 Average Loss: 0.5075, Average Hypernet Loss: 110.0949, Accuracy: 87.22%, Hypernet Accuracy: 80.13%\n",
      "Validation Epoch: 1 Average Loss: 0.5361, Accuracy: 87.89%\n",
      "Hypernet Validation Epoch: 1 Average Loss: 8.1128, Accuracy: 88.38%\n",
      "Train Epoch: 2 [6336/48000 (13%)]\tLoss: 0.476816. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [12736/48000 (27%)]\tLoss: 0.422676. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [19136/48000 (40%)]\tLoss: 0.742011. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [25536/48000 (53%)]\tLoss: 0.490679. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [31936/48000 (67%)]\tLoss: 0.609973. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [38336/48000 (80%)]\tLoss: 0.413967. Hypernet Loss: 2.302585\n",
      "Train Epoch: 2 [44736/48000 (93%)]\tLoss: 0.547941. Hypernet Loss: 97242.695312\n",
      "Train Epoch: 2 Average Loss: 0.5039, Average Hypernet Loss: 15495.3204, Accuracy: 88.65%, Hypernet Accuracy: 34.86%\n",
      "Validation Epoch: 2 Average Loss: 0.4928, Accuracy: 88.88%\n",
      "Hypernet Validation Epoch: 2 Average Loss: 2857.0901, Accuracy: 79.42%\n",
      "Train Epoch: 3 [6336/48000 (13%)]\tLoss: 0.266077. Hypernet Loss: 5.285717\n",
      "Train Epoch: 3 [12736/48000 (27%)]\tLoss: 1.185148. Hypernet Loss: 2.497630\n",
      "Train Epoch: 3 [19136/48000 (40%)]\tLoss: 0.971712. Hypernet Loss: 0.858185\n",
      "Train Epoch: 3 [25536/48000 (53%)]\tLoss: 0.544680. Hypernet Loss: 53.876640\n",
      "Train Epoch: 3 [31936/48000 (67%)]\tLoss: 0.294096. Hypernet Loss: 1076.014038\n",
      "Train Epoch: 3 [38336/48000 (80%)]\tLoss: 0.228270. Hypernet Loss: 0.660744\n",
      "Train Epoch: 3 [44736/48000 (93%)]\tLoss: 0.605274. Hypernet Loss: 0.877922\n",
      "Train Epoch: 3 Average Loss: 0.5033, Average Hypernet Loss: 1169.3970, Accuracy: 88.59%, Hypernet Accuracy: 75.46%\n",
      "Validation Epoch: 3 Average Loss: 0.6241, Accuracy: 86.57%\n",
      "Hypernet Validation Epoch: 3 Average Loss: 437.1692, Accuracy: 68.50%\n",
      "Train Epoch: 4 [6336/48000 (13%)]\tLoss: 0.425107. Hypernet Loss: 0.716873\n",
      "Train Epoch: 4 [12736/48000 (27%)]\tLoss: 0.378209. Hypernet Loss: 1.194148\n",
      "Train Epoch: 4 [19136/48000 (40%)]\tLoss: 0.872200. Hypernet Loss: 1.752097\n",
      "Train Epoch: 4 [25536/48000 (53%)]\tLoss: 0.599692. Hypernet Loss: 3.127646\n",
      "Train Epoch: 4 [31936/48000 (67%)]\tLoss: 0.483674. Hypernet Loss: 0.676218\n",
      "Train Epoch: 4 [38336/48000 (80%)]\tLoss: 0.257486. Hypernet Loss: 0.615818\n",
      "Train Epoch: 4 [44736/48000 (93%)]\tLoss: 0.523146. Hypernet Loss: 901.205139\n",
      "Train Epoch: 4 Average Loss: 0.5212, Average Hypernet Loss: 860.2617, Accuracy: 88.64%, Hypernet Accuracy: 72.41%\n",
      "Validation Epoch: 4 Average Loss: 0.5616, Accuracy: 88.51%\n",
      "Hypernet Validation Epoch: 4 Average Loss: 0.8500, Accuracy: 77.17%\n",
      "Train Epoch: 5 [6336/48000 (13%)]\tLoss: 0.345522. Hypernet Loss: 0.738581\n",
      "Train Epoch: 5 [12736/48000 (27%)]\tLoss: 0.687524. Hypernet Loss: 1.067694\n",
      "Train Epoch: 5 [19136/48000 (40%)]\tLoss: 0.647636. Hypernet Loss: 55.147800\n",
      "Train Epoch: 5 [25536/48000 (53%)]\tLoss: 0.562059. Hypernet Loss: 0.683181\n",
      "Train Epoch: 5 [31936/48000 (67%)]\tLoss: 0.759037. Hypernet Loss: 0.331776\n",
      "Train Epoch: 5 [38336/48000 (80%)]\tLoss: 0.637718. Hypernet Loss: 0.582517\n",
      "Train Epoch: 5 [44736/48000 (93%)]\tLoss: 0.920160. Hypernet Loss: 0.394226\n",
      "Train Epoch: 5 Average Loss: 0.5246, Average Hypernet Loss: 119.9608, Accuracy: 88.48%, Hypernet Accuracy: 79.55%\n",
      "Validation Epoch: 5 Average Loss: 0.5155, Accuracy: 89.65%\n",
      "Hypernet Validation Epoch: 5 Average Loss: 46.1842, Accuracy: 84.07%\n",
      "Train Epoch: 6 [6336/48000 (13%)]\tLoss: 0.110372. Hypernet Loss: 0.252320\n",
      "Train Epoch: 6 [12736/48000 (27%)]\tLoss: 0.616259. Hypernet Loss: 0.683199\n",
      "Train Epoch: 6 [19136/48000 (40%)]\tLoss: 0.339546. Hypernet Loss: 0.418619\n",
      "Train Epoch: 6 [25536/48000 (53%)]\tLoss: 0.283179. Hypernet Loss: 0.534085\n",
      "Train Epoch: 6 [31936/48000 (67%)]\tLoss: 0.514463. Hypernet Loss: 0.731782\n",
      "Train Epoch: 6 [38336/48000 (80%)]\tLoss: 0.840257. Hypernet Loss: 0.587046\n",
      "Train Epoch: 6 [44736/48000 (93%)]\tLoss: 0.481422. Hypernet Loss: 0.720764\n",
      "Train Epoch: 6 Average Loss: 0.5274, Average Hypernet Loss: 285.9629, Accuracy: 88.74%, Hypernet Accuracy: 78.67%\n",
      "Validation Epoch: 6 Average Loss: 0.5889, Accuracy: 88.34%\n",
      "Hypernet Validation Epoch: 6 Average Loss: 104.1286, Accuracy: 82.72%\n",
      "Train Epoch: 7 [6336/48000 (13%)]\tLoss: 0.739291. Hypernet Loss: 0.647579\n",
      "Train Epoch: 7 [12736/48000 (27%)]\tLoss: 0.455879. Hypernet Loss: 0.419440\n",
      "Train Epoch: 7 [19136/48000 (40%)]\tLoss: 0.540603. Hypernet Loss: 0.581118\n",
      "Train Epoch: 7 [25536/48000 (53%)]\tLoss: 0.516002. Hypernet Loss: 0.730509\n",
      "Train Epoch: 7 [31936/48000 (67%)]\tLoss: 0.453811. Hypernet Loss: 0.317877\n",
      "Train Epoch: 7 [38336/48000 (80%)]\tLoss: 0.568772. Hypernet Loss: 0.253161\n",
      "Train Epoch: 7 [44736/48000 (93%)]\tLoss: 0.962016. Hypernet Loss: 0.631210\n",
      "Train Epoch: 7 Average Loss: 0.5001, Average Hypernet Loss: 662.4160, Accuracy: 88.81%, Hypernet Accuracy: 82.94%\n",
      "Validation Epoch: 7 Average Loss: 0.5902, Accuracy: 87.72%\n",
      "Hypernet Validation Epoch: 7 Average Loss: 274.9595, Accuracy: 89.12%\n",
      "Train Epoch: 8 [6336/48000 (13%)]\tLoss: 0.158714. Hypernet Loss: 0.526281\n",
      "Train Epoch: 8 [12736/48000 (27%)]\tLoss: 0.599026. Hypernet Loss: 0.597568\n",
      "Train Epoch: 8 [19136/48000 (40%)]\tLoss: 0.826102. Hypernet Loss: 1.220413\n",
      "Train Epoch: 8 [25536/48000 (53%)]\tLoss: 0.593642. Hypernet Loss: 1.132906\n",
      "Train Epoch: 8 [31936/48000 (67%)]\tLoss: 0.553569. Hypernet Loss: 1.098117\n",
      "Train Epoch: 8 [38336/48000 (80%)]\tLoss: 0.527821. Hypernet Loss: 1.086392\n",
      "Train Epoch: 8 [44736/48000 (93%)]\tLoss: 0.231844. Hypernet Loss: 0.838268\n",
      "Train Epoch: 8 Average Loss: 0.5100, Average Hypernet Loss: 10.8788, Accuracy: 88.80%, Hypernet Accuracy: 72.25%\n",
      "Validation Epoch: 8 Average Loss: 0.5042, Accuracy: 88.92%\n",
      "Hypernet Validation Epoch: 8 Average Loss: 0.7190, Accuracy: 76.48%\n",
      "Train Epoch: 9 [6336/48000 (13%)]\tLoss: 0.447865. Hypernet Loss: 0.583945\n",
      "Train Epoch: 9 [12736/48000 (27%)]\tLoss: 0.708194. Hypernet Loss: 0.464373\n",
      "Train Epoch: 9 [19136/48000 (40%)]\tLoss: 0.380682. Hypernet Loss: 102.302139\n",
      "Train Epoch: 9 [25536/48000 (53%)]\tLoss: 0.813860. Hypernet Loss: 0.746171\n",
      "Train Epoch: 9 [31936/48000 (67%)]\tLoss: 1.121000. Hypernet Loss: 0.644282\n",
      "Train Epoch: 9 [38336/48000 (80%)]\tLoss: 0.824876. Hypernet Loss: 0.552657\n",
      "Train Epoch: 9 [44736/48000 (93%)]\tLoss: 0.562229. Hypernet Loss: 0.518580\n",
      "Train Epoch: 9 Average Loss: 0.5063, Average Hypernet Loss: 99.8136, Accuracy: 88.72%, Hypernet Accuracy: 82.84%\n",
      "Validation Epoch: 9 Average Loss: 0.5624, Accuracy: 88.28%\n",
      "Hypernet Validation Epoch: 9 Average Loss: 0.5542, Accuracy: 84.04%\n",
      "Train Epoch: 10 [6336/48000 (13%)]\tLoss: 0.531196. Hypernet Loss: 0.306284\n",
      "Train Epoch: 10 [12736/48000 (27%)]\tLoss: 0.492816. Hypernet Loss: 0.515739\n",
      "Train Epoch: 10 [19136/48000 (40%)]\tLoss: 0.955549. Hypernet Loss: 0.736692\n",
      "Train Epoch: 10 [25536/48000 (53%)]\tLoss: 0.237439. Hypernet Loss: 0.576179\n",
      "Train Epoch: 10 [31936/48000 (67%)]\tLoss: 0.278162. Hypernet Loss: 0.447329\n",
      "Train Epoch: 10 [38336/48000 (80%)]\tLoss: 0.671568. Hypernet Loss: 0.415262\n",
      "Train Epoch: 10 [44736/48000 (93%)]\tLoss: 0.339221. Hypernet Loss: 0.461229\n",
      "Train Epoch: 10 Average Loss: 0.5297, Average Hypernet Loss: 23.8377, Accuracy: 88.68%, Hypernet Accuracy: 82.46%\n",
      "Validation Epoch: 10 Average Loss: 0.5496, Accuracy: 88.20%\n",
      "Hypernet Validation Epoch: 10 Average Loss: 0.5285, Accuracy: 84.59%\n",
      "Train Epoch: 11 [6336/48000 (13%)]\tLoss: 0.643383. Hypernet Loss: 0.642481\n",
      "Train Epoch: 11 [12736/48000 (27%)]\tLoss: 0.400698. Hypernet Loss: 0.476429\n",
      "Train Epoch: 11 [19136/48000 (40%)]\tLoss: 0.657600. Hypernet Loss: 0.595236\n",
      "Train Epoch: 11 [25536/48000 (53%)]\tLoss: 0.533954. Hypernet Loss: 0.437845\n",
      "Train Epoch: 11 [31936/48000 (67%)]\tLoss: 0.303229. Hypernet Loss: 0.607838\n",
      "Train Epoch: 11 [38336/48000 (80%)]\tLoss: 0.495667. Hypernet Loss: 0.442988\n",
      "Train Epoch: 11 [44736/48000 (93%)]\tLoss: 1.083687. Hypernet Loss: 0.655082\n",
      "Train Epoch: 11 Average Loss: 0.5166, Average Hypernet Loss: 188.9993, Accuracy: 88.73%, Hypernet Accuracy: 78.69%\n",
      "Validation Epoch: 11 Average Loss: 0.6367, Accuracy: 86.00%\n",
      "Hypernet Validation Epoch: 11 Average Loss: 116.1273, Accuracy: 85.70%\n",
      "Train Epoch: 12 [6336/48000 (13%)]\tLoss: 0.998448. Hypernet Loss: 0.714992\n",
      "Train Epoch: 12 [12736/48000 (27%)]\tLoss: 0.807589. Hypernet Loss: 0.592418\n",
      "Train Epoch: 12 [19136/48000 (40%)]\tLoss: 0.386301. Hypernet Loss: 0.238594\n",
      "Train Epoch: 12 [25536/48000 (53%)]\tLoss: 0.557275. Hypernet Loss: 0.344473\n",
      "Train Epoch: 12 [31936/48000 (67%)]\tLoss: 0.444401. Hypernet Loss: 0.435560\n",
      "Train Epoch: 12 [38336/48000 (80%)]\tLoss: 0.456925. Hypernet Loss: 0.511169\n",
      "Train Epoch: 12 [44736/48000 (93%)]\tLoss: 0.667665. Hypernet Loss: 0.412564\n",
      "Train Epoch: 12 Average Loss: 0.5160, Average Hypernet Loss: 92.6729, Accuracy: 88.66%, Hypernet Accuracy: 83.89%\n",
      "Validation Epoch: 12 Average Loss: 0.5748, Accuracy: 88.14%\n",
      "Hypernet Validation Epoch: 12 Average Loss: 37.5377, Accuracy: 87.05%\n",
      "Train Epoch: 13 [6336/48000 (13%)]\tLoss: 0.323934. Hypernet Loss: 0.285071\n",
      "Train Epoch: 13 [12736/48000 (27%)]\tLoss: 0.366922. Hypernet Loss: 0.299073\n",
      "Train Epoch: 13 [19136/48000 (40%)]\tLoss: 0.402126. Hypernet Loss: 0.654771\n",
      "Train Epoch: 13 [25536/48000 (53%)]\tLoss: 0.554363. Hypernet Loss: 0.486150\n",
      "Train Epoch: 13 [31936/48000 (67%)]\tLoss: 0.658425. Hypernet Loss: 0.676390\n",
      "Train Epoch: 13 [38336/48000 (80%)]\tLoss: 0.176247. Hypernet Loss: 0.259636\n",
      "Train Epoch: 13 [44736/48000 (93%)]\tLoss: 0.569329. Hypernet Loss: 0.453028\n",
      "Train Epoch: 13 Average Loss: 0.5080, Average Hypernet Loss: 47.0663, Accuracy: 88.76%, Hypernet Accuracy: 83.49%\n",
      "Validation Epoch: 13 Average Loss: 0.5406, Accuracy: 88.61%\n",
      "Hypernet Validation Epoch: 13 Average Loss: 0.3841, Accuracy: 88.90%\n",
      "Train Epoch: 14 [6336/48000 (13%)]\tLoss: 0.461355. Hypernet Loss: 0.490821\n",
      "Train Epoch: 14 [12736/48000 (27%)]\tLoss: 0.512624. Hypernet Loss: 0.665658\n",
      "Train Epoch: 14 [19136/48000 (40%)]\tLoss: 0.325591. Hypernet Loss: 1.236475\n",
      "Train Epoch: 14 [25536/48000 (53%)]\tLoss: 0.106447. Hypernet Loss: 0.246782\n",
      "Train Epoch: 14 [31936/48000 (67%)]\tLoss: 0.730874. Hypernet Loss: 0.890158\n",
      "Train Epoch: 14 [38336/48000 (80%)]\tLoss: 0.130711. Hypernet Loss: 0.487435\n",
      "Train Epoch: 14 [44736/48000 (93%)]\tLoss: 0.478052. Hypernet Loss: 0.640860\n",
      "Train Epoch: 14 Average Loss: 0.5105, Average Hypernet Loss: 196.9680, Accuracy: 88.75%, Hypernet Accuracy: 79.98%\n",
      "Validation Epoch: 14 Average Loss: 0.5532, Accuracy: 88.81%\n",
      "Hypernet Validation Epoch: 14 Average Loss: 0.7376, Accuracy: 80.53%\n",
      "Train Epoch: 15 [6336/48000 (13%)]\tLoss: 0.258034. Hypernet Loss: 7.683429\n",
      "Train Epoch: 15 [12736/48000 (27%)]\tLoss: 0.460394. Hypernet Loss: 1.004217\n",
      "Train Epoch: 15 [19136/48000 (40%)]\tLoss: 0.475526. Hypernet Loss: 0.588918\n",
      "Train Epoch: 15 [25536/48000 (53%)]\tLoss: 0.388475. Hypernet Loss: 2035.433472\n",
      "Train Epoch: 15 [31936/48000 (67%)]\tLoss: 0.214232. Hypernet Loss: 398.071503\n",
      "Train Epoch: 15 [38336/48000 (80%)]\tLoss: 0.420317. Hypernet Loss: 5.416775\n",
      "Train Epoch: 15 [44736/48000 (93%)]\tLoss: 0.336720. Hypernet Loss: 0.803236\n",
      "Train Epoch: 15 Average Loss: 0.5024, Average Hypernet Loss: 1011.4615, Accuracy: 88.81%, Hypernet Accuracy: 71.21%\n",
      "Validation Epoch: 15 Average Loss: 0.5045, Accuracy: 89.97%\n",
      "Hypernet Validation Epoch: 15 Average Loss: 0.4278, Accuracy: 88.36%\n",
      "Train Epoch: 16 [6336/48000 (13%)]\tLoss: 0.527961. Hypernet Loss: 0.390070\n",
      "Train Epoch: 16 [12736/48000 (27%)]\tLoss: 0.597317. Hypernet Loss: 0.409729\n",
      "Train Epoch: 16 [19136/48000 (40%)]\tLoss: 0.358942. Hypernet Loss: 0.459162\n",
      "Train Epoch: 16 [25536/48000 (53%)]\tLoss: 0.358690. Hypernet Loss: 9166.174805\n",
      "Train Epoch: 16 [31936/48000 (67%)]\tLoss: 0.800239. Hypernet Loss: 0.765243\n",
      "Train Epoch: 16 [38336/48000 (80%)]\tLoss: 0.696805. Hypernet Loss: 0.568243\n",
      "Train Epoch: 16 [44736/48000 (93%)]\tLoss: 0.591009. Hypernet Loss: 0.466413\n",
      "Train Epoch: 16 Average Loss: 0.5153, Average Hypernet Loss: 149.3915, Accuracy: 88.77%, Hypernet Accuracy: 83.18%\n",
      "Validation Epoch: 16 Average Loss: 0.4970, Accuracy: 88.64%\n",
      "Hypernet Validation Epoch: 16 Average Loss: 0.4067, Accuracy: 88.28%\n",
      "Train Epoch: 17 [6336/48000 (13%)]\tLoss: 0.534990. Hypernet Loss: 0.598700\n",
      "Train Epoch: 17 [12736/48000 (27%)]\tLoss: 0.182810. Hypernet Loss: 0.606874\n",
      "Train Epoch: 17 [19136/48000 (40%)]\tLoss: 0.317484. Hypernet Loss: 0.439621\n",
      "Train Epoch: 17 [25536/48000 (53%)]\tLoss: 0.578400. Hypernet Loss: 0.368744\n",
      "Train Epoch: 17 [31936/48000 (67%)]\tLoss: 0.337580. Hypernet Loss: 0.365273\n",
      "Train Epoch: 17 [38336/48000 (80%)]\tLoss: 0.851919. Hypernet Loss: 0.583771\n",
      "Train Epoch: 17 [44736/48000 (93%)]\tLoss: 0.715636. Hypernet Loss: 8.467014\n",
      "Train Epoch: 17 Average Loss: 0.5217, Average Hypernet Loss: 217.4162, Accuracy: 88.58%, Hypernet Accuracy: 81.09%\n",
      "Validation Epoch: 17 Average Loss: 0.5912, Accuracy: 88.06%\n",
      "Hypernet Validation Epoch: 17 Average Loss: 0.6059, Accuracy: 84.88%\n",
      "Train Epoch: 18 [6336/48000 (13%)]\tLoss: 0.786686. Hypernet Loss: 1.359843\n",
      "Train Epoch: 18 [12736/48000 (27%)]\tLoss: 0.701197. Hypernet Loss: 0.437310\n",
      "Train Epoch: 18 [19136/48000 (40%)]\tLoss: 0.072154. Hypernet Loss: 0.137133\n",
      "Train Epoch: 18 [25536/48000 (53%)]\tLoss: 0.484380. Hypernet Loss: 0.504242\n",
      "Train Epoch: 18 [31936/48000 (67%)]\tLoss: 0.543798. Hypernet Loss: 1.011594\n",
      "Train Epoch: 18 [38336/48000 (80%)]\tLoss: 0.187066. Hypernet Loss: 0.542464\n",
      "Train Epoch: 18 [44736/48000 (93%)]\tLoss: 0.721012. Hypernet Loss: 0.564164\n",
      "Train Epoch: 18 Average Loss: 0.5194, Average Hypernet Loss: 176.3040, Accuracy: 88.61%, Hypernet Accuracy: 79.06%\n",
      "Validation Epoch: 18 Average Loss: 0.5196, Accuracy: 89.08%\n",
      "Hypernet Validation Epoch: 18 Average Loss: 0.9058, Accuracy: 73.66%\n",
      "Train Epoch: 19 [6336/48000 (13%)]\tLoss: 0.332800. Hypernet Loss: 0.368399\n",
      "Train Epoch: 19 [12736/48000 (27%)]\tLoss: 0.402858. Hypernet Loss: 0.693250\n",
      "Train Epoch: 19 [19136/48000 (40%)]\tLoss: 0.616535. Hypernet Loss: 0.606180\n",
      "Train Epoch: 19 [25536/48000 (53%)]\tLoss: 0.613896. Hypernet Loss: 0.636294\n",
      "Train Epoch: 19 [31936/48000 (67%)]\tLoss: 0.352661. Hypernet Loss: 0.443444\n",
      "Train Epoch: 19 [38336/48000 (80%)]\tLoss: 0.811422. Hypernet Loss: 0.568504\n",
      "Train Epoch: 19 [44736/48000 (93%)]\tLoss: 0.302383. Hypernet Loss: 1.962103\n",
      "Train Epoch: 19 Average Loss: 0.4930, Average Hypernet Loss: 25.6683, Accuracy: 88.91%, Hypernet Accuracy: 81.53%\n",
      "Validation Epoch: 19 Average Loss: 0.5214, Accuracy: 88.45%\n",
      "Hypernet Validation Epoch: 19 Average Loss: 0.7227, Accuracy: 78.83%\n",
      "Train Epoch: 20 [6336/48000 (13%)]\tLoss: 0.309341. Hypernet Loss: 0.524323\n",
      "Train Epoch: 20 [12736/48000 (27%)]\tLoss: 0.276484. Hypernet Loss: 0.517364\n",
      "Train Epoch: 20 [19136/48000 (40%)]\tLoss: 0.502498. Hypernet Loss: 0.493689\n",
      "Train Epoch: 20 [25536/48000 (53%)]\tLoss: 0.710424. Hypernet Loss: 1.596134\n",
      "Train Epoch: 20 [31936/48000 (67%)]\tLoss: 0.363983. Hypernet Loss: 0.539107\n",
      "Train Epoch: 20 [38336/48000 (80%)]\tLoss: 0.217208. Hypernet Loss: 0.272488\n",
      "Train Epoch: 20 [44736/48000 (93%)]\tLoss: 0.400013. Hypernet Loss: 0.417623\n",
      "Train Epoch: 20 Average Loss: 0.4990, Average Hypernet Loss: 539.1539, Accuracy: 88.70%, Hypernet Accuracy: 80.30%\n",
      "Validation Epoch: 20 Average Loss: 0.5772, Accuracy: 88.37%\n",
      "Hypernet Validation Epoch: 20 Average Loss: 96.1545, Accuracy: 16.39%\n",
      "Train Epoch: 21 [6336/48000 (13%)]\tLoss: 1.201565. Hypernet Loss: 0.811418\n",
      "Train Epoch: 21 [12736/48000 (27%)]\tLoss: 0.235300. Hypernet Loss: 0.395455\n",
      "Train Epoch: 21 [19136/48000 (40%)]\tLoss: 0.425373. Hypernet Loss: 0.488856\n",
      "Train Epoch: 21 [25536/48000 (53%)]\tLoss: 0.177307. Hypernet Loss: 0.293350\n",
      "Train Epoch: 21 [31936/48000 (67%)]\tLoss: 0.643163. Hypernet Loss: 1114.036743\n",
      "Train Epoch: 21 [38336/48000 (80%)]\tLoss: 0.369464. Hypernet Loss: 339.084167\n",
      "Train Epoch: 21 [44736/48000 (93%)]\tLoss: 0.392913. Hypernet Loss: 0.239730\n",
      "Train Epoch: 21 Average Loss: 0.5072, Average Hypernet Loss: 59.1461, Accuracy: 88.63%, Hypernet Accuracy: 82.00%\n",
      "Validation Epoch: 21 Average Loss: 0.6411, Accuracy: 87.53%\n",
      "Hypernet Validation Epoch: 21 Average Loss: 1.1917, Accuracy: 89.03%\n",
      "Train Epoch: 22 [6336/48000 (13%)]\tLoss: 0.501660. Hypernet Loss: 0.445506\n",
      "Train Epoch: 22 [12736/48000 (27%)]\tLoss: 0.592303. Hypernet Loss: 0.885550\n",
      "Train Epoch: 22 [19136/48000 (40%)]\tLoss: 0.436299. Hypernet Loss: 0.454006\n",
      "Train Epoch: 22 [25536/48000 (53%)]\tLoss: 0.723718. Hypernet Loss: 0.918737\n",
      "Train Epoch: 22 [31936/48000 (67%)]\tLoss: 0.474182. Hypernet Loss: 0.454134\n",
      "Train Epoch: 22 [38336/48000 (80%)]\tLoss: 0.463959. Hypernet Loss: 1.391454\n",
      "Train Epoch: 22 [44736/48000 (93%)]\tLoss: 0.629417. Hypernet Loss: 0.823855\n",
      "Train Epoch: 22 Average Loss: 0.5047, Average Hypernet Loss: 225.3739, Accuracy: 88.83%, Hypernet Accuracy: 79.76%\n",
      "Validation Epoch: 22 Average Loss: 0.5043, Accuracy: 89.44%\n",
      "Hypernet Validation Epoch: 22 Average Loss: 0.7542, Accuracy: 77.10%\n",
      "Train Epoch: 23 [6336/48000 (13%)]\tLoss: 0.564637. Hypernet Loss: 1.349655\n",
      "Train Epoch: 23 [12736/48000 (27%)]\tLoss: 0.582360. Hypernet Loss: 0.438851\n",
      "Train Epoch: 23 [19136/48000 (40%)]\tLoss: 0.112222. Hypernet Loss: 0.811848\n",
      "Train Epoch: 23 [25536/48000 (53%)]\tLoss: 0.533768. Hypernet Loss: 0.314004\n",
      "Train Epoch: 23 [31936/48000 (67%)]\tLoss: 0.455689. Hypernet Loss: 1.033121\n",
      "Train Epoch: 23 [38336/48000 (80%)]\tLoss: 0.546531. Hypernet Loss: 0.383652\n",
      "Train Epoch: 23 [44736/48000 (93%)]\tLoss: 0.939069. Hypernet Loss: 1.710793\n",
      "Train Epoch: 23 Average Loss: 0.5227, Average Hypernet Loss: 79.3277, Accuracy: 88.44%, Hypernet Accuracy: 79.78%\n",
      "Validation Epoch: 23 Average Loss: 0.6988, Accuracy: 84.82%\n",
      "Hypernet Validation Epoch: 23 Average Loss: 0.4902, Accuracy: 85.25%\n",
      "Train Epoch: 24 [6336/48000 (13%)]\tLoss: 0.902373. Hypernet Loss: 0.872909\n",
      "Train Epoch: 24 [12736/48000 (27%)]\tLoss: 0.570879. Hypernet Loss: 0.428748\n",
      "Train Epoch: 24 [19136/48000 (40%)]\tLoss: 1.335690. Hypernet Loss: 0.824001\n",
      "Train Epoch: 24 [25536/48000 (53%)]\tLoss: 0.555041. Hypernet Loss: 0.686211\n",
      "Train Epoch: 24 [31936/48000 (67%)]\tLoss: 0.811532. Hypernet Loss: 0.463695\n",
      "Train Epoch: 24 [38336/48000 (80%)]\tLoss: 0.287158. Hypernet Loss: 0.267668\n",
      "Train Epoch: 24 [44736/48000 (93%)]\tLoss: 0.600704. Hypernet Loss: 1.556928\n",
      "Train Epoch: 24 Average Loss: 0.5044, Average Hypernet Loss: 75.1151, Accuracy: 88.69%, Hypernet Accuracy: 81.98%\n",
      "Validation Epoch: 24 Average Loss: 0.5993, Accuracy: 87.46%\n",
      "Hypernet Validation Epoch: 24 Average Loss: 1.1656, Accuracy: 85.57%\n",
      "Train Epoch: 25 [6336/48000 (13%)]\tLoss: 0.113614. Hypernet Loss: 0.157843\n",
      "Train Epoch: 25 [12736/48000 (27%)]\tLoss: 0.677648. Hypernet Loss: 0.705682\n",
      "Train Epoch: 25 [19136/48000 (40%)]\tLoss: 0.824986. Hypernet Loss: 0.843273\n",
      "Train Epoch: 25 [25536/48000 (53%)]\tLoss: 0.548737. Hypernet Loss: 0.667135\n",
      "Train Epoch: 25 [31936/48000 (67%)]\tLoss: 0.345470. Hypernet Loss: 0.274182\n",
      "Train Epoch: 25 [38336/48000 (80%)]\tLoss: 0.495988. Hypernet Loss: 0.521879\n",
      "Train Epoch: 25 [44736/48000 (93%)]\tLoss: 0.162864. Hypernet Loss: 0.175028\n",
      "Train Epoch: 25 Average Loss: 0.5002, Average Hypernet Loss: 58.7323, Accuracy: 88.75%, Hypernet Accuracy: 80.57%\n",
      "Validation Epoch: 25 Average Loss: 0.6620, Accuracy: 86.91%\n",
      "Hypernet Validation Epoch: 25 Average Loss: 0.4220, Accuracy: 87.67%\n",
      "Train Epoch: 26 [6336/48000 (13%)]\tLoss: 0.531959. Hypernet Loss: 0.414389\n",
      "Train Epoch: 26 [12736/48000 (27%)]\tLoss: 0.594182. Hypernet Loss: 0.420740\n",
      "Train Epoch: 26 [19136/48000 (40%)]\tLoss: 0.462957. Hypernet Loss: 3.374036\n",
      "Train Epoch: 26 [25536/48000 (53%)]\tLoss: 0.521897. Hypernet Loss: 0.708050\n",
      "Train Epoch: 26 [31936/48000 (67%)]\tLoss: 0.198162. Hypernet Loss: 0.207916\n",
      "Train Epoch: 26 [38336/48000 (80%)]\tLoss: 0.529808. Hypernet Loss: 0.452649\n",
      "Train Epoch: 26 [44736/48000 (93%)]\tLoss: 0.794836. Hypernet Loss: 0.908454\n",
      "Train Epoch: 26 Average Loss: 0.5200, Average Hypernet Loss: 250.2928, Accuracy: 88.75%, Hypernet Accuracy: 78.40%\n",
      "Validation Epoch: 26 Average Loss: 0.5136, Accuracy: 89.66%\n",
      "Hypernet Validation Epoch: 26 Average Loss: 0.5402, Accuracy: 83.75%\n",
      "Train Epoch: 27 [6336/48000 (13%)]\tLoss: 0.479525. Hypernet Loss: 0.683801\n",
      "Train Epoch: 27 [12736/48000 (27%)]\tLoss: 0.823037. Hypernet Loss: 0.789986\n",
      "Train Epoch: 27 [19136/48000 (40%)]\tLoss: 0.465628. Hypernet Loss: 1.923407\n",
      "Train Epoch: 27 [25536/48000 (53%)]\tLoss: 0.353794. Hypernet Loss: 0.299672\n",
      "Train Epoch: 27 [31936/48000 (67%)]\tLoss: 0.673627. Hypernet Loss: 1.494873\n",
      "Train Epoch: 27 [38336/48000 (80%)]\tLoss: 0.240732. Hypernet Loss: 0.875294\n",
      "Train Epoch: 27 [44736/48000 (93%)]\tLoss: 0.715845. Hypernet Loss: 0.787882\n",
      "Train Epoch: 27 Average Loss: 0.5120, Average Hypernet Loss: 174.8474, Accuracy: 88.65%, Hypernet Accuracy: 76.80%\n",
      "Validation Epoch: 27 Average Loss: 0.5599, Accuracy: 87.93%\n",
      "Hypernet Validation Epoch: 27 Average Loss: 2.0549, Accuracy: 86.84%\n",
      "Train Epoch: 28 [6336/48000 (13%)]\tLoss: 0.340158. Hypernet Loss: 0.828510\n",
      "Train Epoch: 28 [12736/48000 (27%)]\tLoss: 0.230029. Hypernet Loss: 0.499403\n",
      "Train Epoch: 28 [19136/48000 (40%)]\tLoss: 0.303667. Hypernet Loss: 0.647530\n",
      "Train Epoch: 28 [25536/48000 (53%)]\tLoss: 0.891718. Hypernet Loss: 0.684846\n",
      "Train Epoch: 28 [31936/48000 (67%)]\tLoss: 0.906212. Hypernet Loss: 0.510300\n",
      "Train Epoch: 28 [38336/48000 (80%)]\tLoss: 0.617902. Hypernet Loss: 0.513081\n",
      "Train Epoch: 28 [44736/48000 (93%)]\tLoss: 0.784304. Hypernet Loss: 0.553031\n",
      "Train Epoch: 28 Average Loss: 0.5311, Average Hypernet Loss: 57.0928, Accuracy: 88.50%, Hypernet Accuracy: 79.84%\n",
      "Validation Epoch: 28 Average Loss: 0.6438, Accuracy: 86.26%\n",
      "Hypernet Validation Epoch: 28 Average Loss: 199.9911, Accuracy: 84.62%\n",
      "Train Epoch: 29 [6336/48000 (13%)]\tLoss: 0.282405. Hypernet Loss: 0.510886\n",
      "Train Epoch: 29 [12736/48000 (27%)]\tLoss: 0.252053. Hypernet Loss: 0.157846\n",
      "Train Epoch: 29 [19136/48000 (40%)]\tLoss: 0.302331. Hypernet Loss: 0.369655\n",
      "Train Epoch: 29 [25536/48000 (53%)]\tLoss: 0.512053. Hypernet Loss: 0.323488\n",
      "Train Epoch: 29 [31936/48000 (67%)]\tLoss: 0.144979. Hypernet Loss: 1.081053\n",
      "Train Epoch: 29 [38336/48000 (80%)]\tLoss: 0.336792. Hypernet Loss: 100.153740\n",
      "Train Epoch: 29 [44736/48000 (93%)]\tLoss: 0.339690. Hypernet Loss: 0.671745\n",
      "Train Epoch: 29 Average Loss: 0.4960, Average Hypernet Loss: 113.1899, Accuracy: 88.88%, Hypernet Accuracy: 78.33%\n",
      "Validation Epoch: 29 Average Loss: 0.6029, Accuracy: 87.22%\n",
      "Hypernet Validation Epoch: 29 Average Loss: 149.4047, Accuracy: 38.62%\n",
      "Train Epoch: 30 [6336/48000 (13%)]\tLoss: 0.675895. Hypernet Loss: 0.625212\n",
      "Train Epoch: 30 [12736/48000 (27%)]\tLoss: 0.515028. Hypernet Loss: 0.467580\n",
      "Train Epoch: 30 [19136/48000 (40%)]\tLoss: 0.508816. Hypernet Loss: 0.283750\n",
      "Train Epoch: 30 [25536/48000 (53%)]\tLoss: 0.240496. Hypernet Loss: 0.301815\n",
      "Train Epoch: 30 [31936/48000 (67%)]\tLoss: 0.469213. Hypernet Loss: 1.069408\n",
      "Train Epoch: 30 [38336/48000 (80%)]\tLoss: 0.576438. Hypernet Loss: 0.547573\n",
      "Train Epoch: 30 [44736/48000 (93%)]\tLoss: 0.831505. Hypernet Loss: 0.896366\n",
      "Train Epoch: 30 Average Loss: 0.5045, Average Hypernet Loss: 53.3704, Accuracy: 88.78%, Hypernet Accuracy: 80.05%\n",
      "Validation Epoch: 30 Average Loss: 0.6066, Accuracy: 88.45%\n",
      "Hypernet Validation Epoch: 30 Average Loss: 1.1795, Accuracy: 63.18%\n",
      "Train Epoch: 31 [6336/48000 (13%)]\tLoss: 0.439441. Hypernet Loss: 0.458523\n",
      "Train Epoch: 31 [12736/48000 (27%)]\tLoss: 0.971825. Hypernet Loss: 0.727673\n",
      "Train Epoch: 31 [19136/48000 (40%)]\tLoss: 0.600674. Hypernet Loss: 0.753576\n",
      "Train Epoch: 31 [25536/48000 (53%)]\tLoss: 0.158243. Hypernet Loss: 0.183684\n",
      "Train Epoch: 31 [31936/48000 (67%)]\tLoss: 0.293997. Hypernet Loss: 2.302585\n",
      "Train Epoch: 31 [38336/48000 (80%)]\tLoss: 0.551269. Hypernet Loss: 0.569371\n",
      "Train Epoch: 31 [44736/48000 (93%)]\tLoss: 0.669230. Hypernet Loss: 0.468259\n",
      "Train Epoch: 31 Average Loss: 0.5211, Average Hypernet Loss: 145.4601, Accuracy: 88.65%, Hypernet Accuracy: 78.95%\n",
      "Validation Epoch: 31 Average Loss: 0.5939, Accuracy: 88.81%\n",
      "Hypernet Validation Epoch: 31 Average Loss: 0.5148, Accuracy: 84.26%\n",
      "Train Epoch: 32 [6336/48000 (13%)]\tLoss: 0.972167. Hypernet Loss: 0.483617\n",
      "Train Epoch: 32 [12736/48000 (27%)]\tLoss: 0.461909. Hypernet Loss: 0.668940\n",
      "Train Epoch: 32 [19136/48000 (40%)]\tLoss: 0.492324. Hypernet Loss: 0.752794\n",
      "Train Epoch: 32 [25536/48000 (53%)]\tLoss: 0.861437. Hypernet Loss: 2.302585\n",
      "Train Epoch: 32 [31936/48000 (67%)]\tLoss: 0.751633. Hypernet Loss: 0.467674\n",
      "Train Epoch: 32 [38336/48000 (80%)]\tLoss: 0.587245. Hypernet Loss: 0.562365\n",
      "Train Epoch: 32 [44736/48000 (93%)]\tLoss: 0.748081. Hypernet Loss: 0.718523\n",
      "Train Epoch: 32 Average Loss: 0.5056, Average Hypernet Loss: 83.8857, Accuracy: 88.88%, Hypernet Accuracy: 77.19%\n",
      "Validation Epoch: 32 Average Loss: 0.5771, Accuracy: 87.99%\n",
      "Hypernet Validation Epoch: 32 Average Loss: 0.5334, Accuracy: 84.13%\n",
      "Train Epoch: 33 [6336/48000 (13%)]\tLoss: 0.871193. Hypernet Loss: 0.520196\n",
      "Train Epoch: 33 [12736/48000 (27%)]\tLoss: 1.022979. Hypernet Loss: 2.224713\n",
      "Train Epoch: 33 [19136/48000 (40%)]\tLoss: 0.419990. Hypernet Loss: 0.504717\n",
      "Train Epoch: 33 [25536/48000 (53%)]\tLoss: 0.342377. Hypernet Loss: 0.359761\n",
      "Train Epoch: 33 [31936/48000 (67%)]\tLoss: 0.990587. Hypernet Loss: 1.275762\n",
      "Train Epoch: 33 [38336/48000 (80%)]\tLoss: 0.679986. Hypernet Loss: 0.732026\n",
      "Train Epoch: 33 [44736/48000 (93%)]\tLoss: 0.749169. Hypernet Loss: 0.468202\n",
      "Train Epoch: 33 Average Loss: 0.5110, Average Hypernet Loss: 58.4975, Accuracy: 88.60%, Hypernet Accuracy: 78.35%\n",
      "Validation Epoch: 33 Average Loss: 0.6209, Accuracy: 87.71%\n",
      "Hypernet Validation Epoch: 33 Average Loss: 0.4407, Accuracy: 87.59%\n",
      "Train Epoch: 34 [6336/48000 (13%)]\tLoss: 0.572325. Hypernet Loss: 0.289963\n",
      "Train Epoch: 34 [12736/48000 (27%)]\tLoss: 0.795563. Hypernet Loss: 0.445390\n",
      "Train Epoch: 34 [19136/48000 (40%)]\tLoss: 0.401841. Hypernet Loss: 206.691177\n",
      "Train Epoch: 34 [25536/48000 (53%)]\tLoss: 0.973501. Hypernet Loss: 0.651480\n",
      "Train Epoch: 34 [31936/48000 (67%)]\tLoss: 0.549310. Hypernet Loss: 0.649426\n",
      "Train Epoch: 34 [38336/48000 (80%)]\tLoss: 0.836208. Hypernet Loss: 556.935120\n",
      "Train Epoch: 34 [44736/48000 (93%)]\tLoss: 0.464570. Hypernet Loss: 2.302585\n",
      "Train Epoch: 34 Average Loss: 0.5140, Average Hypernet Loss: 51.6787, Accuracy: 88.61%, Hypernet Accuracy: 79.27%\n",
      "Validation Epoch: 34 Average Loss: 0.6120, Accuracy: 88.12%\n",
      "Hypernet Validation Epoch: 34 Average Loss: 0.4985, Accuracy: 84.88%\n",
      "Train Epoch: 35 [6336/48000 (13%)]\tLoss: 0.919824. Hypernet Loss: 0.756761\n",
      "Train Epoch: 35 [12736/48000 (27%)]\tLoss: 0.761891. Hypernet Loss: 0.592797\n",
      "Train Epoch: 35 [19136/48000 (40%)]\tLoss: 0.812299. Hypernet Loss: 0.452747\n",
      "Train Epoch: 35 [25536/48000 (53%)]\tLoss: 0.658725. Hypernet Loss: 0.282943\n",
      "Train Epoch: 35 [31936/48000 (67%)]\tLoss: 0.026213. Hypernet Loss: 0.163552\n",
      "Train Epoch: 35 [38336/48000 (80%)]\tLoss: 0.311093. Hypernet Loss: 0.329439\n",
      "Train Epoch: 35 [44736/48000 (93%)]\tLoss: 0.287611. Hypernet Loss: 0.339538\n",
      "Train Epoch: 35 Average Loss: 0.5209, Average Hypernet Loss: 120.5124, Accuracy: 88.57%, Hypernet Accuracy: 83.40%\n",
      "Validation Epoch: 35 Average Loss: 0.5774, Accuracy: 88.27%\n",
      "Hypernet Validation Epoch: 35 Average Loss: 41.6785, Accuracy: 87.03%\n",
      "Train Epoch: 36 [6336/48000 (13%)]\tLoss: 0.563335. Hypernet Loss: 0.357051\n",
      "Train Epoch: 36 [12736/48000 (27%)]\tLoss: 0.269855. Hypernet Loss: 0.393784\n",
      "Train Epoch: 36 [19136/48000 (40%)]\tLoss: 0.390869. Hypernet Loss: 1.479884\n",
      "Train Epoch: 36 [25536/48000 (53%)]\tLoss: 0.845575. Hypernet Loss: 0.690320\n",
      "Train Epoch: 36 [31936/48000 (67%)]\tLoss: 0.679973. Hypernet Loss: 0.997995\n",
      "Train Epoch: 36 [38336/48000 (80%)]\tLoss: 0.865965. Hypernet Loss: 2.864264\n",
      "Train Epoch: 36 [44736/48000 (93%)]\tLoss: 0.816608. Hypernet Loss: 0.551827\n",
      "Train Epoch: 36 Average Loss: 0.4999, Average Hypernet Loss: 771.8066, Accuracy: 88.79%, Hypernet Accuracy: 79.74%\n",
      "Validation Epoch: 36 Average Loss: 0.5202, Accuracy: 89.22%\n",
      "Hypernet Validation Epoch: 36 Average Loss: 1.0587, Accuracy: 71.48%\n",
      "Train Epoch: 37 [6336/48000 (13%)]\tLoss: 0.405747. Hypernet Loss: 76.675537\n",
      "Train Epoch: 37 [12736/48000 (27%)]\tLoss: 0.357197. Hypernet Loss: 0.624474\n",
      "Train Epoch: 37 [19136/48000 (40%)]\tLoss: 0.194744. Hypernet Loss: 0.514424\n",
      "Train Epoch: 37 [25536/48000 (53%)]\tLoss: 0.323304. Hypernet Loss: 0.305019\n",
      "Train Epoch: 37 [31936/48000 (67%)]\tLoss: 0.604240. Hypernet Loss: 0.507116\n",
      "Train Epoch: 37 [38336/48000 (80%)]\tLoss: 0.739746. Hypernet Loss: 0.706505\n",
      "Train Epoch: 37 [44736/48000 (93%)]\tLoss: 0.325594. Hypernet Loss: 0.755342\n",
      "Train Epoch: 37 Average Loss: 0.4903, Average Hypernet Loss: 192.5749, Accuracy: 88.85%, Hypernet Accuracy: 77.85%\n",
      "Validation Epoch: 37 Average Loss: 0.5977, Accuracy: 87.87%\n",
      "Hypernet Validation Epoch: 37 Average Loss: 0.6961, Accuracy: 78.02%\n",
      "Train Epoch: 38 [6336/48000 (13%)]\tLoss: 0.435267. Hypernet Loss: 0.272299\n",
      "Train Epoch: 38 [12736/48000 (27%)]\tLoss: 0.530352. Hypernet Loss: 0.453131\n",
      "Train Epoch: 38 [19136/48000 (40%)]\tLoss: 0.165221. Hypernet Loss: 1.077616\n",
      "Train Epoch: 38 [25536/48000 (53%)]\tLoss: 0.305760. Hypernet Loss: 0.774168\n",
      "Train Epoch: 38 [31936/48000 (67%)]\tLoss: 0.977166. Hypernet Loss: 0.731957\n",
      "Train Epoch: 38 [38336/48000 (80%)]\tLoss: 0.949440. Hypernet Loss: 0.970214\n",
      "Train Epoch: 38 [44736/48000 (93%)]\tLoss: 0.764573. Hypernet Loss: 0.607400\n",
      "Train Epoch: 38 Average Loss: 0.5136, Average Hypernet Loss: 158.5161, Accuracy: 88.59%, Hypernet Accuracy: 77.95%\n",
      "Validation Epoch: 38 Average Loss: 0.5283, Accuracy: 89.09%\n",
      "Hypernet Validation Epoch: 38 Average Loss: 0.6425, Accuracy: 82.15%\n",
      "Train Epoch: 39 [6336/48000 (13%)]\tLoss: 0.908093. Hypernet Loss: 0.646993\n",
      "Train Epoch: 39 [12736/48000 (27%)]\tLoss: 0.596063. Hypernet Loss: 0.260346\n",
      "Train Epoch: 39 [19136/48000 (40%)]\tLoss: 0.416929. Hypernet Loss: 0.458280\n",
      "Train Epoch: 39 [25536/48000 (53%)]\tLoss: 0.482551. Hypernet Loss: 0.407439\n",
      "Train Epoch: 39 [31936/48000 (67%)]\tLoss: 0.651985. Hypernet Loss: 0.540709\n",
      "Train Epoch: 39 [38336/48000 (80%)]\tLoss: 0.402593. Hypernet Loss: 14.252301\n",
      "Train Epoch: 39 [44736/48000 (93%)]\tLoss: 0.792537. Hypernet Loss: 0.589684\n",
      "Train Epoch: 39 Average Loss: 0.5169, Average Hypernet Loss: 115.5893, Accuracy: 88.67%, Hypernet Accuracy: 80.90%\n",
      "Validation Epoch: 39 Average Loss: 0.5446, Accuracy: 88.67%\n",
      "Hypernet Validation Epoch: 39 Average Loss: 0.4896, Accuracy: 85.91%\n",
      "Train Epoch: 40 [6336/48000 (13%)]\tLoss: 1.369603. Hypernet Loss: 0.669569\n",
      "Train Epoch: 40 [12736/48000 (27%)]\tLoss: 0.755389. Hypernet Loss: 0.696231\n",
      "Train Epoch: 40 [19136/48000 (40%)]\tLoss: 0.489557. Hypernet Loss: 0.436492\n",
      "Train Epoch: 40 [25536/48000 (53%)]\tLoss: 0.719062. Hypernet Loss: 0.478071\n",
      "Train Epoch: 40 [31936/48000 (67%)]\tLoss: 0.682481. Hypernet Loss: 0.918430\n",
      "Train Epoch: 40 [38336/48000 (80%)]\tLoss: 0.217810. Hypernet Loss: 0.298219\n",
      "Train Epoch: 40 [44736/48000 (93%)]\tLoss: 0.223148. Hypernet Loss: 0.312431\n",
      "Train Epoch: 40 Average Loss: 0.5136, Average Hypernet Loss: 216.4907, Accuracy: 88.69%, Hypernet Accuracy: 79.45%\n",
      "Validation Epoch: 40 Average Loss: 0.5312, Accuracy: 89.10%\n",
      "Hypernet Validation Epoch: 40 Average Loss: 83.7476, Accuracy: 15.54%\n",
      "Train Epoch: 41 [6336/48000 (13%)]\tLoss: 0.544319. Hypernet Loss: 0.362019\n",
      "Train Epoch: 41 [12736/48000 (27%)]\tLoss: 0.361763. Hypernet Loss: 2.437279\n",
      "Train Epoch: 41 [19136/48000 (40%)]\tLoss: 0.493067. Hypernet Loss: 0.413369\n",
      "Train Epoch: 41 [25536/48000 (53%)]\tLoss: 0.379249. Hypernet Loss: 649.575317\n",
      "Train Epoch: 41 [31936/48000 (67%)]\tLoss: 0.500680. Hypernet Loss: 0.318589\n",
      "Train Epoch: 41 [38336/48000 (80%)]\tLoss: 0.575061. Hypernet Loss: 0.334995\n",
      "Train Epoch: 41 [44736/48000 (93%)]\tLoss: 0.273489. Hypernet Loss: 0.436701\n",
      "Train Epoch: 41 Average Loss: 0.5130, Average Hypernet Loss: 176.2161, Accuracy: 88.81%, Hypernet Accuracy: 82.29%\n",
      "Validation Epoch: 41 Average Loss: 0.5740, Accuracy: 88.16%\n",
      "Hypernet Validation Epoch: 41 Average Loss: 0.4542, Accuracy: 86.12%\n",
      "Train Epoch: 42 [6336/48000 (13%)]\tLoss: 0.154546. Hypernet Loss: 0.223512\n",
      "Train Epoch: 42 [12736/48000 (27%)]\tLoss: 0.717071. Hypernet Loss: 0.607306\n",
      "Train Epoch: 42 [19136/48000 (40%)]\tLoss: 0.216970. Hypernet Loss: 2.302585\n",
      "Train Epoch: 42 [25536/48000 (53%)]\tLoss: 0.591961. Hypernet Loss: 2.302585\n",
      "Train Epoch: 42 [31936/48000 (67%)]\tLoss: 0.334001. Hypernet Loss: 0.418816\n",
      "Train Epoch: 42 [38336/48000 (80%)]\tLoss: 0.704078. Hypernet Loss: 0.369297\n",
      "Train Epoch: 42 [44736/48000 (93%)]\tLoss: 0.985658. Hypernet Loss: 0.472614\n",
      "Train Epoch: 42 Average Loss: 0.5090, Average Hypernet Loss: 24.1127, Accuracy: 88.59%, Hypernet Accuracy: 81.30%\n",
      "Validation Epoch: 42 Average Loss: 0.5513, Accuracy: 88.84%\n",
      "Hypernet Validation Epoch: 42 Average Loss: 90.7964, Accuracy: 87.11%\n",
      "Train Epoch: 43 [6336/48000 (13%)]\tLoss: 0.260506. Hypernet Loss: 0.341850\n",
      "Train Epoch: 43 [12736/48000 (27%)]\tLoss: 0.570541. Hypernet Loss: 0.326799\n",
      "Train Epoch: 43 [19136/48000 (40%)]\tLoss: 0.453869. Hypernet Loss: 10.353219\n",
      "Train Epoch: 43 [25536/48000 (53%)]\tLoss: 0.373835. Hypernet Loss: 0.655409\n",
      "Train Epoch: 43 [31936/48000 (67%)]\tLoss: 0.249651. Hypernet Loss: 0.495692\n",
      "Train Epoch: 43 [38336/48000 (80%)]\tLoss: 0.309911. Hypernet Loss: 0.313455\n",
      "Train Epoch: 43 [44736/48000 (93%)]\tLoss: 0.260352. Hypernet Loss: 0.682986\n",
      "Train Epoch: 43 Average Loss: 0.5220, Average Hypernet Loss: 45.5792, Accuracy: 88.61%, Hypernet Accuracy: 80.54%\n",
      "Validation Epoch: 43 Average Loss: 0.5525, Accuracy: 88.38%\n",
      "Hypernet Validation Epoch: 43 Average Loss: 0.3950, Accuracy: 88.86%\n",
      "Train Epoch: 44 [6336/48000 (13%)]\tLoss: 0.224325. Hypernet Loss: 1.637607\n",
      "Train Epoch: 44 [12736/48000 (27%)]\tLoss: 0.587161. Hypernet Loss: 0.290019\n",
      "Train Epoch: 44 [19136/48000 (40%)]\tLoss: 1.140082. Hypernet Loss: 0.482222\n",
      "Train Epoch: 44 [25536/48000 (53%)]\tLoss: 0.298878. Hypernet Loss: 0.603495\n",
      "Train Epoch: 44 [31936/48000 (67%)]\tLoss: 0.602975. Hypernet Loss: 0.434405\n",
      "Train Epoch: 44 [38336/48000 (80%)]\tLoss: 0.258694. Hypernet Loss: 4.035845\n",
      "Train Epoch: 44 [44736/48000 (93%)]\tLoss: 0.398666. Hypernet Loss: 0.285393\n",
      "Train Epoch: 44 Average Loss: 0.5046, Average Hypernet Loss: 155.1336, Accuracy: 88.71%, Hypernet Accuracy: 81.58%\n",
      "Validation Epoch: 44 Average Loss: 0.5786, Accuracy: 88.48%\n",
      "Hypernet Validation Epoch: 44 Average Loss: 0.8566, Accuracy: 80.30%\n",
      "Train Epoch: 45 [6336/48000 (13%)]\tLoss: 0.593998. Hypernet Loss: 0.525168\n",
      "Train Epoch: 45 [12736/48000 (27%)]\tLoss: 0.396766. Hypernet Loss: 0.506669\n",
      "Train Epoch: 45 [19136/48000 (40%)]\tLoss: 0.737593. Hypernet Loss: 0.723097\n",
      "Train Epoch: 45 [25536/48000 (53%)]\tLoss: 0.268786. Hypernet Loss: 0.464893\n",
      "Train Epoch: 45 [31936/48000 (67%)]\tLoss: 0.737301. Hypernet Loss: 0.399856\n",
      "Train Epoch: 45 [38336/48000 (80%)]\tLoss: 0.808568. Hypernet Loss: 0.500381\n",
      "Train Epoch: 45 [44736/48000 (93%)]\tLoss: 0.990985. Hypernet Loss: 1.952343\n",
      "Train Epoch: 45 Average Loss: 0.5103, Average Hypernet Loss: 56.0778, Accuracy: 88.73%, Hypernet Accuracy: 82.24%\n",
      "Validation Epoch: 45 Average Loss: 0.5309, Accuracy: 89.28%\n",
      "Hypernet Validation Epoch: 45 Average Loss: 0.6018, Accuracy: 84.52%\n",
      "Train Epoch: 46 [6336/48000 (13%)]\tLoss: 0.541026. Hypernet Loss: 1.241696\n",
      "Train Epoch: 46 [12736/48000 (27%)]\tLoss: 0.588620. Hypernet Loss: 0.406929\n",
      "Train Epoch: 46 [19136/48000 (40%)]\tLoss: 0.204806. Hypernet Loss: 0.715351\n",
      "Train Epoch: 46 [25536/48000 (53%)]\tLoss: 0.565062. Hypernet Loss: 1.255430\n",
      "Train Epoch: 46 [31936/48000 (67%)]\tLoss: 1.037162. Hypernet Loss: 0.854671\n",
      "Train Epoch: 46 [38336/48000 (80%)]\tLoss: 0.763610. Hypernet Loss: 0.487438\n",
      "Train Epoch: 46 [44736/48000 (93%)]\tLoss: 0.329552. Hypernet Loss: 0.349806\n",
      "Train Epoch: 46 Average Loss: 0.5146, Average Hypernet Loss: 115.6905, Accuracy: 88.58%, Hypernet Accuracy: 81.67%\n",
      "Validation Epoch: 46 Average Loss: 0.6094, Accuracy: 86.33%\n",
      "Hypernet Validation Epoch: 46 Average Loss: 0.4590, Accuracy: 86.67%\n",
      "Train Epoch: 47 [6336/48000 (13%)]\tLoss: 0.434447. Hypernet Loss: 0.419200\n",
      "Train Epoch: 47 [12736/48000 (27%)]\tLoss: 0.241757. Hypernet Loss: 0.318174\n",
      "Train Epoch: 47 [19136/48000 (40%)]\tLoss: 0.291193. Hypernet Loss: 0.287209\n",
      "Train Epoch: 47 [25536/48000 (53%)]\tLoss: 0.445238. Hypernet Loss: 0.335115\n",
      "Train Epoch: 47 [31936/48000 (67%)]\tLoss: 0.654981. Hypernet Loss: 0.536769\n",
      "Train Epoch: 47 [38336/48000 (80%)]\tLoss: 0.135315. Hypernet Loss: 0.361160\n",
      "Train Epoch: 47 [44736/48000 (93%)]\tLoss: 0.732407. Hypernet Loss: 0.499411\n",
      "Train Epoch: 47 Average Loss: 0.4998, Average Hypernet Loss: 93.4370, Accuracy: 88.84%, Hypernet Accuracy: 78.74%\n",
      "Validation Epoch: 47 Average Loss: 0.5365, Accuracy: 88.79%\n",
      "Hypernet Validation Epoch: 47 Average Loss: 17.6303, Accuracy: 72.38%\n",
      "Train Epoch: 48 [6336/48000 (13%)]\tLoss: 0.282591. Hypernet Loss: 0.320395\n",
      "Train Epoch: 48 [12736/48000 (27%)]\tLoss: 0.296793. Hypernet Loss: 0.234460\n",
      "Train Epoch: 48 [19136/48000 (40%)]\tLoss: 0.271947. Hypernet Loss: 0.528840\n",
      "Train Epoch: 48 [25536/48000 (53%)]\tLoss: 0.224099. Hypernet Loss: 0.163104\n",
      "Train Epoch: 48 [31936/48000 (67%)]\tLoss: 0.273484. Hypernet Loss: 0.292436\n",
      "Train Epoch: 48 [38336/48000 (80%)]\tLoss: 0.953673. Hypernet Loss: 0.632694\n",
      "Train Epoch: 48 [44736/48000 (93%)]\tLoss: 0.818362. Hypernet Loss: 0.498036\n",
      "Train Epoch: 48 Average Loss: 0.5165, Average Hypernet Loss: 70.5151, Accuracy: 88.59%, Hypernet Accuracy: 80.63%\n",
      "Validation Epoch: 48 Average Loss: 0.7468, Accuracy: 84.63%\n",
      "Hypernet Validation Epoch: 48 Average Loss: 5.7391, Accuracy: 85.87%\n",
      "Train Epoch: 49 [6336/48000 (13%)]\tLoss: 0.401595. Hypernet Loss: 6.229037\n",
      "Train Epoch: 49 [12736/48000 (27%)]\tLoss: 0.440253. Hypernet Loss: 855.452637\n",
      "Train Epoch: 49 [19136/48000 (40%)]\tLoss: 0.533908. Hypernet Loss: 2.302576\n",
      "Train Epoch: 49 [25536/48000 (53%)]\tLoss: 0.222222. Hypernet Loss: 0.231559\n",
      "Train Epoch: 49 [31936/48000 (67%)]\tLoss: 0.640083. Hypernet Loss: 0.284973\n",
      "Train Epoch: 49 [38336/48000 (80%)]\tLoss: 0.600756. Hypernet Loss: 0.396720\n",
      "Train Epoch: 49 [44736/48000 (93%)]\tLoss: 0.325217. Hypernet Loss: 0.224553\n",
      "Train Epoch: 49 Average Loss: 0.5216, Average Hypernet Loss: 126.2412, Accuracy: 88.61%, Hypernet Accuracy: 81.75%\n",
      "Validation Epoch: 49 Average Loss: 0.6020, Accuracy: 87.51%\n",
      "Hypernet Validation Epoch: 49 Average Loss: 0.4369, Accuracy: 88.19%\n",
      "Train Epoch: 50 [6336/48000 (13%)]\tLoss: 0.604106. Hypernet Loss: 0.396311\n",
      "Train Epoch: 50 [12736/48000 (27%)]\tLoss: 0.661254. Hypernet Loss: 0.362875\n",
      "Train Epoch: 50 [19136/48000 (40%)]\tLoss: 0.292185. Hypernet Loss: 2.302585\n",
      "Train Epoch: 50 [25536/48000 (53%)]\tLoss: 0.400380. Hypernet Loss: 0.403131\n",
      "Train Epoch: 50 [31936/48000 (67%)]\tLoss: 0.537360. Hypernet Loss: 271.643768\n",
      "Train Epoch: 50 [38336/48000 (80%)]\tLoss: 0.813295. Hypernet Loss: 0.481635\n",
      "Train Epoch: 50 [44736/48000 (93%)]\tLoss: 0.272735. Hypernet Loss: 0.401365\n",
      "Train Epoch: 50 Average Loss: 0.5033, Average Hypernet Loss: 50.9038, Accuracy: 88.81%, Hypernet Accuracy: 77.96%\n",
      "Validation Epoch: 50 Average Loss: 0.5274, Accuracy: 89.39%\n",
      "Hypernet Validation Epoch: 50 Average Loss: 0.4360, Accuracy: 87.09%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28  # 784 input features (28x28 pixels)\n",
    "output_size = 10     # 10 output classes (digits 0-9)\n",
    "\n",
    "model = Classifier(input_size, output_size)\n",
    "hypernet_model = HyperNetClassifier(input_size, output_size)\n",
    "\n",
    "# --- Initialize the Optimizer ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "hypernet_optimizer = optim.Adam(hypernet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    _, _, _, _ = train(model, hypernet_model, train_loader, optimizer, hypernet_optimizer, epoch)\n",
    "    \n",
    "    val_loss, val_accuracy = evaluate(model, val_loader)\n",
    "    print(f'Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    val_loss, val_accuracy = evaluate(hypernet_model, val_loader)\n",
    "    print(f'Hypernet Validation Epoch: {epoch} Average Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "554bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Testing...\n",
      "Normal Model:\n",
      "Test set: Average loss: 0.4439, Accuracy: 90.39%\n",
      "Hypernet Model:\n",
      "Test set: Average loss: 0.3949, Accuracy: 88.34%\n"
     ]
    }
   ],
   "source": [
    "# --- Testing the Model ---\n",
    "print(\"\\nStarting Testing...\")\n",
    "\n",
    "print(\"Normal Model:\")\n",
    "test(model, test_loader)\n",
    "\n",
    "print(\"Hypernet Model:\")\n",
    "test(hypernet_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
